%%
%% Instructions for processing this document into a PDF document
%%
%% FIRST NOTE, you need the 'biclust' package installed in R.
%% Some of the examples will come from data sets in this package.
%% You can install it from the R console with, for example:
%%
%% install.packages("biclust",repos="http://cran.case.edu")
%%
%% Now you can prepare the PDF of the paper with any of:
%%
%% From a shell:
%% R CMD Sweave --pdf draft.Rnw
%%
%% From an R prompt:
%% Sweave("draft.Rnw")
%% tools::texi2pdf("draft.tex")
%%
%% From RStudio:
%% click on the little "Compile PDF" button
%%
%% Note that you should probably run the above a few times because
%% LaTeX generally takes a few runs to get reference numbers right.
%%
\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcounter{algorithmctr}
\newenvironment{algorithm}{
   \refstepcounter{algorithmctr}
   \bigskip\noindent
   \textbf{Algorithm \thealgorithmctr\\}
}
{\bigskip}
\numberwithin{algorithmctr}{section}


\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}



\title{Efficient Thresholded Correlation using Truncated Singular Value Decomposition}
\author{
James Baglama\\
Department of Mathematics,\\University of Rhode Island
\and
Michael Kane\\
Yale University
\and
Bryan Lewis\\
Paradigm4, Inc.\\
\and
Alex Poliakov\\
Paradigm4, Inc.
}

\date{}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

\begin{abstract}

Efficiently computing a subset of a correlation matrix consisting of values
above a specified threshold is important to many practical applications.
Real-world problems in genomics, machine learning, financial and other time
series and other applications can produce correlation matrices too large to
explicitly form and tractably compute. Often, only values corresponding to
highly-correlated vectors are of interest, and those values typically make up a
small fraction of the overall correlation matrix. We present a method based on
the singular value decomposition (SVD) and its relationship to the data
covariance structure that can efficiently compute thresholded subsets of very
large correlation matrices.

\end{abstract}


\section{Introduction}\label{intro}

Finding highly-correlated pairs among a large set of vectors is an important
part of many applications. For instance, subsets of highly-correlated vectors
of gene expression values may be used in the discovery of networks of genes
relevant to particular biological processes\cite{genomics}.  Identification of
highly-correlated pairs may be used in feature selection algorithms for machine
learning applications\cite{ml1, ml2} and are important to time series and
image processing applications\cite{timeseries, svd-similarity}.

The number of correlation coefficients grows quadratically with the number of
vectors. Simply computing all pairs of correlation coefficients and then
filtering out coefficients below a given threshold may not be computationally
feasable for high-dimensional data in modern genomics and other applications.
Considerable attention has been devoted to \emph{pruning
methods} that cheaply prune away all but the most highly-correlated pairs of
vectors.  For example the TAPER and related algorithms of Xiong et al. develop
pruning rules based on the frequency of mutually co-ocurring observations (rows)
among vector pairs; see \cite{prune1, prune2} and the references therein.
Related methods approximate a set containing the most highly-correlated
pairs using hashing algorithms\cite{prune3}. A number of other methods arrive
at approximate sets of the most highly-correlated pairs using
dimensionality-reduction techniques like the discrete Fourier
transform\cite{timeseries}. The method of Wu, et al.\cite{svd-similarity},
conceptually very similar to our approach, uses the singular value
decomposition (SVD) to find all pairs of close vectors with respect to a
distance metric (instead of correlation).  Our method finds all of the most
highly-correlated vector pairs with respect to a given threshold by pruning
along an intuitive path of decreasing variance defined by the data and computed
by the SVD.


Consider a real-valued data matrix $A$ consisting of $m$ observations of $n$
column vectors.  Denote the columns of $A$ as $a_j\in\mathcal{R}^m$ for
$j=1,2,\ldots,n$, $A=[a_1, a_2, \ldots, a_n]\in\mathcal{R}^{m\times n}$, and
let $k=\mathrm{rank}(A)$.  Assume that the mean of each column $a_j$ is zero
and that each column has unit norm $\|a_j\| = 1$. Here and below, $\|\cdot\|$
denotes the Euclidean vector norm. Then the Pearson sample correlation matrix
$\mathrm{cor}(A)=A^TA$.  Note that under these assumptions the sample
correlation and covariance matrices are the same up to a constant multiple.
Section \ref{irlba} illustrates how to relax the unit norm and zero mean
assumptions in practice.

Let $A=U\Sigma V^T$ be a singular value decomposition of $A$, where
$U\in\mathcal{R}^{m\times k}$, $V\in\mathcal{R}^{n\times k}$,
$U^TU = V^TV = I$,
and $\Sigma\in\mathcal{R}^{k\times k}$ is a diagonal matrix with
diagonal entries $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_k > 0$.  For
later convenience, we write $s_{1:p}=[\sigma_1, \sigma_2, \ldots, \sigma_p]^T$
to be the vector of the first $p\le k$ singular values along the diagonal of
$\Sigma$.  Denote the columns of $V$ as $V=[v_1, v_2, \ldots, v_k]$, where each
$v_j\in\mathcal{R}^n$, $j=1,2,\ldots,k$.  Then $\mathrm{cor}(A) = A^TA =
V\Sigma^2 V^T$ is a symmetric eigenvalue decomposition of the correlation
matrix with nonzero eigenvalues $\sigma_1^2, \sigma_2^2, \ldots, \sigma_k^2$.
The span of the columns of $V$ form an orthonormal basis of the range of the
correlation matrix $A^TA$.

Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix $A$
(including possibly the case $i=j$). Denote the correlation of these two
vectors by $\mathrm{cor}(a_i,a_j) = a_j^T a_i$.  The following simple lemma
establishes a relationship between correlation and Euclidean distance between
two vectors.

\begin{lemma}
\label{distlemma}
Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix
$A$. Then
\begin{equation}\label{cordist}
\mathrm{cor}(a_i,a_j) = a_i^Ta_j = 1 - \|a_i - a_j\|^2/2.
\end{equation}
In particular, for a given correlation threshold $t$, $0<t<1$, if
$\|a_i - a_j\|^2 > 2(1-t)$ then
$\mathrm{cor}(a_i, a_j) < t$.
\end{lemma}
%%\begin{proof}
%%\begin{eqnarray*}
%%\|a_i - a_j\|^2 &=& (a_i - a_j)^T(a_i - a_j) \\
%%&=& a_i^T a_i + a_j^T a_j - 2a_i^T a_j \\
%%&=& \|a_i\|^2 + \|a_j\|^2 - 2a_i^T a_j \\
%%&=& 2 - 2a_i^T a_j.
%%\end{eqnarray*}
%%The lemma follows by solving for $a_i^Ta_j$.
%%\end{proof}

Lemma \ref{distlemma} equates the problem of finding pairs of highly correlated
column vectors from the matrix $A$ with a problem of finding pairs of vectors
sufficiently close together. For example, correlation values larger than 0.99
may only be associated with column vectors from the matrix $A$ whose Euclidean
distance is less than $\sqrt{0.02}$.

The following lemma expresses the Euclidean distance between any two columns of
the matrix $A$ as a weighted sum of projected interval distances along the SVD
basis vectors forming the columns of $V$.
\begin{lemma}
Let $a_i$ and $a_j$
be any two columns of the matrix
$A$, $1\le i,j\le n$. Then
\begin{equation}\label{vdist}
\|a_i - a_j\|^2 =
\sigma_1^2 (e_i^Tv_{1} - e_j^Tv_{1})^2 + 
\sigma_2^2 (e_i^Tv_{2} - e_j^Tv_{2})^2 + \cdots + 
\sigma_k^2 (e_i^Tv_{k} - e_j^Tv_{k})^2,
\end{equation}
\end{lemma}
where here and below, $e_{j}\in\mathcal{R}^n$ represents the $j$th unit basis
vector consisting of $1$ in position $j$ and zeros otherwise.
(Thus, $e_j^Tv_{k}$ is the $j$th position in the vector $v_k$--that is, the
$[j,k]$ entry in the matrix $V$.)

%%\begin{proof}
%%\begin{align*}
%%\|a_i - a_j\|^2 &= (a_i - a_j)^T(a_i - a_j) \\
%%&= a_i^Ta_i + a_j^T a_j - 2a_i^Ta_j \\
%%&= \sigma_1^2 v_{1i}^2 + \sigma_2^2 v_{2i}^2 + \cdots + \sigma_k^2 v_{ki}^2 \\
%%&\phantom{=} + \sigma_1^2 v_{1j}^2 + \sigma_2^2 v_{2j}^2 + \cdots + \sigma_k^2 v_{kj}^2 \\
%%&\phantom{=} -2(\sigma_1^2 v_{1i}v_{1j} + \sigma_2^2 v_{2i}v_{2j} + \cdots + \sigma_k^2 v_{ki}v_{kj}) \\
%%&= \sigma_1^2 (v_{1i}^2 + v_{1j}^2 - 2v_{1i}v_{1j}) \\
%%&\phantom{=} + \sigma_2^2 (v_{2i}^2 + v_{2j}^2 - 2v_{2i}v_{2j}) \\
%%&\phantom{=} + \cdots \\
%%&\phantom{=} + \sigma_k^2 (v_{ki}^2 + v_{kj}^2 - 2v_{ki}v_{kj}) \\
%%&= \sigma_1^2 |v_{1i} - v_{1j}|^2
%% + \sigma_2^2 |v_{2i} - v_{2j}|^2
%% + \cdots
%% + \sigma_k^2 |v_{ki} - v_{kj}|^2.
%%\end{align*}
%%\end{proof}

Note that each term in the sum is nonnegative and the weights $\sigma_j^2$,
$j=1,2,\ldots,k$ are nonincreasing and defined by the covariance structure of
the data.  Given a correlation threshold $t$, $0<t<1$, Equations \ref{cordist}
and \ref{vdist} together suggest that pairs of vectors can be excluded from
consideration whenever their projected distance is too large.  For example if
$\sigma_1^2 (e_i^Tv_{1} - e_j^Tv_{1})^2 > 2(1-t)$, then we can conclude from
just a few scalar values that the column vectors $a_i$ and $a_j$ do not meet the
correlation threshold $t$.
In practice, many pairs of vectors may be pruned in this fashion by
inspecting distances in low-dimensional subspaces, substantially reducing the
complexity of computing thresholded correlation matrices.

\section{Efficient pruning using truncated SVD}

Let $t$ be a given correlation threshold, $0<t<1$, and let
$P\in\mathcal{R}^{n\times n}$ be a permutation matrix that orders the entries
of $v_1$ in increasing order.  For example, Figure \ref{fig1} displays the
ordered entries of $Pv_1$ for the small ``BicatYeast'' example (where
$A\in\mathcal{R}^{70\times 419}$) from Section \ref{examples}.  The lines in
the plot illustrate the interval associated with the correlation threshold
$t=0.95$ placed at an arbitrary vertical axis location.

\begin{figure}[!ht]
\begin{center}
<<eval=TRUE, echo=FALSE, fig=TRUE, height=5, width=7>>=
library(biclust)
data(BicatYeast)
A=t(BicatYeast)
s=svd(A)
i=order(s$v[,1])
plot(s$v[i,1], ylab=expression(P * v[1]), xlab="Sorted index", pch='.')
t=0.95
abline(h=c(-0.05,-0.05+sqrt(2*(1-t))/s$d[1]),col=2,lwd=0.5)
@
\caption{
Example plot of the ordered entries of $v_1$ for the BicatYeast example from
Section \ref{examples}. The red lines illustrate the interval corresponding
threshold $t=0.95$ placed at an arbitrary location on the vertical axis. Points
farther apart than this interval correspond to column vectors that do not meet
the correlation threshold.
\label{fig1}
}
\end{center}
\end{figure}

Let $D^i\in\mathcal{R}^{n-i, n}$, $i=1,2,\ldots,n-1$, be the $i$th order finite
difference matrix with $-1$ along the diagonal, $1$ along the $i$th
super-diagonal, and zeros elsewhere.  Consider just adjacent points of $P v_1$.
Then $D^1P v_1$ consists of the differences of adjacent entries in $P v_1$.
Entries such that $\sigma_1^2 (D^1 P v_1)^2 > 2(1-t)$ correspond to pairs of
vectors that do not meet the correlation threshold, where here and below
an exponent applied to a vector denotes element-wise exponentiation.

Even a 1-d projected interval distance threshold may prune many possible
non-adjacent vector pairs with respect to the ordering defined by $P$ as
illustrated by the example in Figure \ref{fig1}.  However, this observation is
unlikely to rule out many adjacent pairs of vectors in most problems.  For
instance, the maximum distance between adjacent points shown in the example in
Figure \ref{fig1} is less than $\sqrt{2(1-t)}/\sigma_1$, and no pruning of
adjacent pairs of vectors with respect to the ordering $P$ occurs.

Including more terms from Equation \ref{vdist} increases our ability to prune
adjacent pairs of vectors
below the correlation threshold
with respect to the ordering defined by $P$.
For example,
including only one more term finds that 49\% of adjacent vectors fall below the
a correlation threshold of $0.95$ in the example shown in Figure \ref{fig1} and
are pruned by identifying indices such that
\[
\sigma_1^2 (D^1 P v_1)^2 +
\sigma_2^2 (D^1 P v_2)^2 > 2(1-t).
\]
Note that we use the permutation $P$ defined by the order of vector $v_1$
throughout. Including ten terms prunes over 96\% of adjacent vector pairs,
leaving only 15 candidate pairs of adjacent vectors that might possibly
meet the correlation threshold.
In general, using $p\le k$ terms to prune pairs of vectors below the
correlation threshold boils down to evaluating the expression
\begin{equation}\label{proj}
(D^1 P V_{1:p} )^2 s_{1:p}^2 > 2(1-t),
\end{equation}
where 
$V_{1:p}$ denotes the first $p$ columns of the
matrix $V$, $s_{1:p}$ the vector of the first $p$ singular values.

Let $\ell$ be the longest run of successive points in $P v_1$ within the
interval $\sqrt{2(1-t)}/\sigma_1$.  The quantity $\ell$ may, for example, be
obtained by rolling the interval over all the points in Figure \ref{fig1} and
counting the maximum number of points that lie within the interval.  With
respect to the ordering $P$, $\ell$ represents the biggest index difference
that pairs of indices corresponding to correlated vectors above the threshold
can exhibit.

Pairs of vectors below the correlation threshold that lie more than one index
difference apart relative to the permutation $P$ can be similarly pruned by
replacing $D^1$ with $D^j$, $j=2,3,\ldots,\ell$ in the above expressions.
Following this process, we can produce a well-pruned candidate set of vector
pairs that contains all pairs that meet a given correlation threshold using
$\ell$ matrix vector products with matrices of order $n \times p$, where $p$ is
chosen in practice such that $p<<k$. Setting $p=10$ for the example shown in
Figure \ref{fig1} prunes all but $431$ possible vector pairs out of a total
$419(419-1)/2 = 87,571$ with $\ell=138$.

These steps are formalized in Algorithm \ref{prune} below. The algorithm
proceeds in two main parts: steps 1--6 prune pairs of vectors down to a small
candidate set that may meet the correlation threshold; step 7
computes the correlation values and applies the threshold across the reduced
set of vector pairs.

\begin{algorithm}\label{prune}
Input: data matrix $A\in \mathcal{R}^{m\times n}$ with columns scaled to have
zero mean and unit norm, correlation threshold $t$,
truncated SVD rank $p<<k$.
\begin{enumerate}
\item Compute the truncated SVD
$AV_{1:p} = U_{1:p}\mathrm{diag}(s_{1:p})$,
where\\
$V_{1:p}=[v_1, v_2, \ldots, v_p]$
and $\mathrm{diag}(s_{1:p})$ denotes the diagonal matrix of the first $p$ singular values.
\item Compute permutation $P$ that orders points in $v_1$.
\item Compute $\ell$, the longest run of successive points in $P v_1$ within the interval $\sqrt{2(1-t)}/\sigma_1$.
\item Compute a set of candidate index pairs $(\tilde{j},\tilde{j}+\tilde{i})$ with respect to the index order defined by the permutation $P$ that possibly meet the correlation threshold
\[
\bigcup_{\tilde{i}=1}^\ell
\left\{
(\tilde{j},\tilde{j}+\tilde{i}) : 
e_{\tilde{j}}^T(D^{\tilde{i}} P V_{1:p} )^2 s_{1:p}^2 \le 2(1-t)
\right\},
\]
where $\tilde{j}=1,2,\ldots,n-\tilde{i}$.
\item If there are judged to be  ``too many'' candidate pairs, increase $p$,
compute $AV_{1:p} = U_{1:p}\mathrm{diag}(s_{1:p})$ and go to step 4.
Continue to increase $p$ in this way until the number of candidate
pairs is sufficiently small or stops decreasing (the best this algorithm can do).
\item Recover the non-permuted candidate column vector
indices $(j,i)$ from $j=e_{\tilde{j}}^T Pq$ and $i=e_{\tilde{j}+\tilde{i}}^TPq$ for every
pair $(\tilde{j},\tilde{j} + \tilde{i})$ in step 4, where
$q\in\mathcal{R}^n = [1,2,\ldots,n]^T$.
\item Compute the full correlation coefficient for each column vector pair identified in
the previous step, applying the correlation threshold to this reduced set of values.
\end{enumerate}
\end{algorithm}
Because the singular values $\sigma_j$, $j=1,2,\ldots,n$ are non-increasing and
decrease in proportion to variability in the data, it's often reasonable to
judge whether there are ``too many'' candidate pairs (step 5) after only a 1-d
projection $\tilde{i}=1$ in step 4 of the algorithm.

Algorithm \ref{prune} guarantees that no pruned vector pair exceeds the given
correlation threshold--the pruning does not produce false negatives.
The algorithm typically prunes the vast majority of pairs of vectors below the
correlation threshold with truncated singular value decompositions of only
relatively low rank $p$.  The choice of $p$ represents a balance between work in
computing a truncated SVD and work required by the $\ell$ matrix vector
products of order $n\times p$ in step 4.



\section{General matrices and fast truncated SVD}\label{irlba}

We have so far assumed that the mean of each column vector of the $m\times n$
matrix $A$ is zero and the columns are scaled to have unit norm, but we really
want a method that works with general matrices $A$.  We also want a way to
efficiently compute a relatively low-rank truncated SVD of potentially large
matrices required by step 1 of Algorithm \ref{prune}.  And the truncated SVD
method should be able to cheaply restart to compute additional singular vectors
as required in step 5 of Algorithm \ref{prune}. Fortunately all desires are met
by one algorithm.

The augmented implicitly restarted Lanczos bidiagonalization algorithm\break(IRLBA)
of Baglama and Reichel \cite{irlba} efficiently computes truncated singular value
decompositions of large dense or sparse matrices. Reference implementations are
available for R\cite{irlbar}, Matlab\cite{irlbam}, and Python\cite{irlbap}.

We can relax the column mean and scale assumptions without introducing much
additional computational or storage overhead as follows. Assume $A$ is an
$m\times n$ real-valued matrix without constant-valued columns. Let
$z=[z_1,z_2,\ldots,z_n]\in\mathcal{R}^n$ represent the vector of column means
of the $m\times n$ matrix $A$, $w=[w_1^2,w_2^2,\ldots,w_n^2]\in\mathcal{R}^n$
be the vector of squared column norms of $A$, and $W\in\mathcal{R}^{n\times n}$
a diagonal matrix with diagonal entries $1/\sqrt{w_i^2 - m z_i^2}$.  Let
$e\in\mathcal{R}^m$ be a vector of all ones.  Then $(A - ez^T)W$ is a centered
matrix with zero column means and scaled to have unit column norms, and
\[
\mathrm{cor}(A) = W^T (A-ez^T)^T (A-ez^T) W.
\]
The IRLBA, based on the Lanczos process, is a method of iterated matrix-vector
products. The main idea behind efficient application of IRLB to correlation
problems replaces matrix-vector products of the form $Ax$ with $AWx - ez^TWx$ in
the iterations, implicitly working with a scaled and centered matrix without
forming it.  This comes at the cost of storing two additional length $n$ vectors and
at most computing two additional vector inner products per matrix-vector product.
The R implementation\cite{irlbar} of IRLBA includes arguments for centering and
scaling the input matrix.


\section{Numerical experiments}\label{examples}

A full/naive thresholded correlation computation requires $O(n^2 m)$ flops. Let
$p$ be the selected IRLBA dimension. Then the proposed method requires
$O(\ell n p)$ flops, where $\ell$ is the longest run of ordered entries in $v_1$
that
meet the 1-d projected distance threshold $\sqrt{2(1-t)}/\sigma_1$,
plus the flops required by IRLBA and the post-processing step of evaluating the
correlation of all the candidate pairs.

%%<<eval=TRUE,echo=FALSE>>=
%%library(irlba)
%%
%%print("R code here")
%%@




\section{NOTES not for publication}

A full/naive thresholded correlation requires $O(n^2 m)$ flops. Let $p$ be the
selected IRLBA dimension. Then the proposed method requires $O(\ell  n  p)$
flops, where $\ell$ is the longest run of ordered entries in $v_1$ that meet
the 1-d projected distance threshold $\sqrt{2(1-t)}/\sigma_1$ (not counting
from flops due to IRLBA and the post-processing step of evaluating the
correlation of all the candidate pairs).  One problem with the method as
written is that $\ell$ can a large fraction of n.

Unnecessarily large values of $\ell$ are due to the loose bound
$\sqrt{2(1-t)}/\sigma_1$ used in step 3 of Algorithm\ref{prune} which is only
sharp when $k=1$. General problems may not need to run through differencing
steps up to $\ell$ in step 4 of the algorithm. We can further improve the
efficiency of the algorithm by dynamically reducing $\ell$ in step 4 as
follows.

Equation \ref{vdist} expresses the Euclidean distance between any two columns
of the matrix $A$ in terms of singular components. Write the smallest squared
gap between entries of a particular singular vector $v_h$ as:
\[
\delta_h = \min_{i,j} (e_i^T v_h - e_j^T v_h)^2.
\]
Then note that
\begin{equation}\label{mindist}
\|a_i - a_j\|^2 \ge \sigma_1^2 \delta_1 + \sigma_2^2 \delta_2 + \cdots + \sigma_k^2\delta_k,
\end{equation}
for any indices $i,j$.

Fix $1\le \tilde{i}\le \ell$ and let $\nu \in R^p$ be the vector of minimum values in
each column of the $n\times p$ matrix $D^{\tilde{i}}PV_{1:p}$ from step 4 of
the algorithm.
It follows from Equation \ref{mindist} that
\[
e_{\tilde{j}}^T(D^{\tilde{i}} P V_{1:p} )^2 s_{1:p}^2 \ge \nu^Ts_{1:p}
\]
for all $\tilde{j}=1,2,\ldots,n$.

XXX need monotonicity of $\nu^Ts_{1:p}$ XXX



\section{The distribution of $||a_i - a_j||^2$}

\begin{lemma}
\label{asymdistm1lemma}
Let $m=1$, $X_i$ and $X_j$ be distributed as $\mathcal{N}(0, \Delta_{ij})$,
with $\Delta_{ii} = 1$.  Then
\begin{equation}
\label{eqasymdist1}
|| X_i - X_j ||^2 \sim R + \left(\Delta_{ij} + 1 \right) S + 
  \left(\Delta_{ij} - 1 \right) T
\end{equation}
where $R$ is distributed as $\chi_2^2$ and $S$ and $T$ are $\chi_1^2$.
\end{lemma}
\begin{proof}
\begin{equation*}
||X_i - X_j||^2 = X_i^2 - 2 X_i X_j + X_j^2
\end{equation*}
$X_i^2$ and $X_j^2$ are clearly $\chi_1^2$. To derive the distribution
of the cross-product term realize
\begin{equation*}
X_i X_j = \frac{1}{4} \left(X_i + X_j\right)^2 - 
  \frac{1}{4} \left(X_i - X_j\right)^2.
\end{equation*}
$X_i + X_j$ and $X_i - X_j$ are both distributed as normal and they are 
independent
since $\mathbf{E}X_1^2 = \mathbf{E}X_2^2$. Therefore, their squares
are $\chi_1^2$ and the leading coefficients in Equation \ref{eqasymdist1}
are the respective variances.
\end{proof}

\begin{lemma}
\label{asymdistlemma}
Let $a_i$ and $a_j$ be any two columns of the matrix 
$B \in \mathcal{R}^{m \times n}$, sampled from 
$\mathcal{N}(0, \Delta/m)$ where $\Delta_{ii} = 1$ for $1 \leq i \leq n$.
Then
\begin{equation*}
||a_i - a_j||^2 \xrightarrow[m]{D} \mathcal{N} \left(2 - 2 \Delta_{ij},  
  \frac{6 + 4\Delta_{ij}^2}{m} \right) 
\end{equation*}
\end{lemma}
This result follows by applying the central limit theorem to
the result in Lemma \ref{asymdistm1lemma} where $m$ is appropriately large.







\section{Conclusion}\label{conclusion}








\begin{thebibliography}{99}

%% Related use of SVD in other methods
%\bibitem{spca}
%Bair, Eric, et al. Prediction by supervised principal components. Journal of the American Statistical Association 101.473 (2006).

%% irlba
\bibitem{irlba}
Baglama, James, and Lothar Reichel. Augmented implicitly restarted Lanczos bidiagonalization methods. SIAM Journal on Scientific Computing 27.1 (2005): 19-42.
\bibitem{irlbam}
Baglama, James,  {\url{http://www.math.uri.edu/~jbaglama/software/irlba.m}}.

%% Applications in machine learning
\bibitem{ml1}
Hall, Mark A. Correlation-based feature selection for machine learning. Diss. The University of Waikato, 1999.

%% IRLBA implementations
\bibitem{irlbap}
Kane, Michael and Lewis, Bryan,  \url{https://github.com/bwlewis/irlbpy}.

%% IRLBA implementations
\bibitem{irlbar}
Lewis, Bryan,  \url{https://cran.r-project.org/web/packages/irlba}.

%% Approximate correlation
\bibitem{timeseries}
Mueen, Abdullah, Suman Nath, and Jie Liu. Fast approximate correlation for massive time-series data. Proceedings of the 2010 ACM SIGMOD International Conference on Management of data. ACM, 2010.

%% Applications in genomics
\bibitem{genomics}
Song, Lin, Peter Langfelder, and Steve Horvath. Comparison of co-expression measures: mutual information, correlation, and model based indices. BMC bioinformatics 13.1 (2012): 328.

%% SVD similarity -- extremely similar to the approach in this paper!
%% Applied to images
\bibitem{svd-similarity}
Wu, Daniel, et al. Efficient retrieval for browsing large image databases. Proceedings of the fifth international conference on Information and knowledge management. ACM, 1996.

%% Other correlation pruning methods
\bibitem{prune1}
Xiong, Hui, et al. TAPER: A two-step approach for all-strong-pairs correlation query in large databases. Knowledge and Data Engineering, IEEE Transactions on 18.4 (2006): 493-508.
\bibitem{prune2}
Xiong, Hui, Mark Brodie, and Sheng Ma. Top-cop: Mining top-k strongly correlated pairs in large databases. Data Mining, 2006. ICDM'06. Sixth International Conference on. IEEE, 2006.

%% Applications in machine learning
\bibitem{ml2}
Yu, Lei, and Huan Liu. Feature selection for high-dimensional data: A fast correlation-based filter solution. ICML. Vol. 3. 2003.

%% Other correlation pruning methods
\bibitem{prune3}
Zhang, Jian, and Joan Feigenbaum. Finding highly correlated pairs efficiently with powerful pruning. Proceedings of the 15th ACM international conference on Information and knowledge management. ACM, 2006.

\end{thebibliography}
\end{document}
