%%
%% Instructions for processing this document into a PDF document
%%
%% FIRST NOTE, you need the 'biclust' package installed in R.
%% Some of the examples will come from data sets in this package.
%% You can install it from the R console with, for example:
%%
%% install.packages("biclust",repos="http://cran.case.edu")
%%
%% Now you can prepare the PDF of the paper with any of:
%%
%% From a shell:
%% R CMD Sweave --pdf draft.Rnw
%%
%% From an R prompt:
%% Sweave("draft.Rnw")
%% tools::texi2pdf("draft.tex")
%%
%% From RStudio:
%% click on the little "Compile PDF" button
%%
%% Note that you should probably run the above a few times because
%% LaTeX generally takes a few runs to get reference numbers right.
%%
\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{hyperref}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcounter{algorithmctr}
\newenvironment{algorithm}{
   \refstepcounter{algorithmctr}
   \bigskip\noindent
   \textbf{Algorithm \thealgorithmctr\\}
}
{\bigskip}
\numberwithin{algorithmctr}{section}


\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}




\title{Efficient Thresholded Correlation using Truncated Singular Value Decomposition}
\author{Some dudes}

\date{}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

\begin{abstract}

Efficiently computing a subset of a correlation matrix consisting of values
above a specified threshold is important to many practical applications.
Real-world problems in genomics, machine learning, finance, and other fields
can produce correlation matrices too large to explicitly form and tracticably
compute. Often, only values corresponding to highly-correlated vectors are of
interest, and those values typically make up a small fraction of the overall
correlation matrix. We present a method based on the singular value
decomposition (SVD) and its relationship to the data covariance structure that
can efficiently compute thresholded subsets of very large correlation matrices.

\end{abstract}



\section{Introduction}\label{intro}

XXX add discussion of applications (genomics?) and previous work,
then on to the preliminaries:
\[\]

Consider a real-valued data matrix $A$ consisting of $m$ observations of $n$
column vectors.  Denote the columns of $A$ as $a_j\in\mathcal{R}^m$ for
$j=1,2,\ldots,n$, $A=[a_1, a_2, \ldots, a_n]\in\mathcal{R}^{m\times n}$, and
let $k=\mathrm{rank}(A)$.  Assume that the mean of each column $a_j$ is zero
and that each column has unit norm $\|a_j\| = 1$. Here and below, $\|\cdot\|$
denotes the Euclidean vector norm. Then the sample correlation matrix
$\mathrm{cor}(A)=A^TA$.  Note that under these assumptions the sample
correlation and covariance matrices are the same.  Section \ref{irlba} below
illustrates how to relax the unit norm and zero mean assumptions in practice.

Let $A=U\Sigma V^T$ be the singular value decomposition of $A$, where
$U\in\mathcal{R}^{m\times k}$, $U^TU = I$, $V\in\mathcal{R}^{n\times k}$,
$V^TV=I$, and $\Sigma\in\mathcal{R}^{k\times k}$ is a diagonal matrix with
diagonal entries $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_k > 0$.  For
later convenience, we write $s_{1:p}=[\sigma_1, \sigma_2, \ldots, \sigma_p]^T$ to be
the vector of the first $p\le k$ singular values along the diagonal of
$\Sigma$.  Denote the columns of $V$ as $V=[v_1, v_2, \ldots, v_k]$, where each
$v_j\in\mathcal{R}^n$, $j=1,2,\ldots,k$.  Then $\mathrm{cor}(A) = A^TA =
V\Sigma^2 V^T$ is a symmetric eigenvalue decomposition of the correlation
matrix with eigenvalues $\sigma_1^2, \sigma_2^2, \ldots, \sigma_k^2$. The span
of the columns of $V$ form an orthonormal basis of the range of the correlation
matrix $A^TA$.

Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix $A$
(including possibly the case $i=j$). Denote the correlation of these two vectors
by $\mathrm{cor}(a_i,a_j) = a_j^T a_i$.  The following simple lemma
from Rafiei\cite{raf} establishes a relationship between correlation and
Euclidean distance between two vectors.

\begin{lemma}
Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix
$A$. Then
\begin{equation}\label{cordist}
\mathrm{cor}(a_i,a_j) = a_i^Ta_j = 1 - \|a_i - a_j\|^2/2.
\end{equation}
In particular, for a given correlation threshold $t$, $0<t<1$, pairs
of vectors $a_i$ and $a_j$ such that $\|a_i - a_j\|^2 > 2(1-t)$ lie
below the correlation threshold.
\end{lemma}
\begin{proof} (XXX perhaps omit this)
\begin{eqnarray*}
\|a_i - a_j\|^2 &=& (a_i - a_j)^T(a_i - a_j) \\
&=& a_i^T a_i + a_j^T a_j - 2a_i^T a_j \\
&=& \|a_i\|^2 + \|a_j\|^2 - 2a_i^T a_j \\
&=& 2 - 2a_i^T a_j.
\end{eqnarray*}
The lemma follows by solving for $a_i^Ta_j$.
\end{proof}

Equation \ref{cordist} equates the problem of finding pairs of highly
correlated column vectors from the matrix $A$ with a problem of finding paris
of vectors sufficiently close together. For example, correlation values larger
than 0.99 may only be associated with column vectors from the matrix $A$ whose
Euclidean distance is less than $\sqrt{0.02}$.  Vectors farther apart will not
meet the correlation threshold.

The following lemma expresses the Euclidean distance between any two columns of the
matrix $A$ as a weighted sum of projected interval distances along the SVD
basis vectors forming the columns of $V$.
\begin{lemma}
Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix
$A$. Then
\begin{equation}\label{vdist}
\|a_i - a_j\|^2 =
\sigma_1^2 |v_{1i} - v_{1j}|^2 + 
\sigma_2^2 |v_{2i} - v_{2j}|^2 + \cdots + 
\sigma_k^2 |v_{ki} - v_{kj}|^2,
\end{equation}
where $v_{kj}$ denotes the $jth$ position in the vector $v_k$ (that is, the
$[k,j]$ entry in the matrix $V$).
\end{lemma}

%%\begin{proof}
%%\begin{align*}
%%\|a_i - a_j\|^2 &= (a_i - a_j)^T(a_i - a_j) \\
%%&= a_i^Ta_i + a_j^T a_j - 2a_i^Ta_j \\
%%&= \sigma_1^2 v_{1i}^2 + \sigma_2^2 v_{2i}^2 + \cdots + \sigma_k^2 v_{ki}^2 \\
%%&\phantom{=} + \sigma_1^2 v_{1j}^2 + \sigma_2^2 v_{2j}^2 + \cdots + \sigma_k^2 v_{kj}^2 \\
%%&\phantom{=} -2(\sigma_1^2 v_{1i}v_{1j} + \sigma_2^2 v_{2i}v_{2j} + \cdots + \sigma_k^2 v_{ki}v_{kj}) \\
%%&= \sigma_1^2 (v_{1i}^2 + v_{1j}^2 - 2v_{1i}v_{1j}) \\
%%&\phantom{=} + \sigma_2^2 (v_{2i}^2 + v_{2j}^2 - 2v_{2i}v_{2j}) \\
%%&\phantom{=} + \cdots \\
%%&\phantom{=} + \sigma_k^2 (v_{ki}^2 + v_{kj}^2 - 2v_{ki}v_{kj}) \\
%%&= \sigma_1^2 |v_{1i} - v_{1j}|^2
%% + \sigma_2^2 |v_{2i} - v_{2j}|^2
%% + \cdots
%% + \sigma_k^2 |v_{ki} - v_{kj}|^2.
%%\end{align*}
%%\end{proof}

Note that each term in the sum is nonnegative and the weights $\sigma_j^2$,
$j=1,2,\ldots,k$ are nonincreasing and defined by the covariance structure of
the data.  Given a correlation threshold $t$, $0<t<1$, Equations \ref{cordist}
and \ref{vdist} together suggest that pairs of vectors can be excluded from
consideration whenever their projected distance is too large.  For example if
$\sigma_1^2 |v_{1i} - v_{1j}|^2 \ge 2(1-t)$, then we can conclude from just two
scalar values that the column vectors $a_i$ and $a_j$ do not meet the
correlation threshold $t$.

In practice, many pairs of vectors may be ruled out in this fashion by
inspecting distances in low-dimensional subspaces, substantially reducing the
complexity of computing thresholded correlation matrices.  Approaches that
cheaply omit pairs of vectors from consideration are called \emph{pruning
methods}, and a variety pruning methods have been described elsewhere for the
thresholded correlation problem; see, for example\cite{zf,mnl} and the
references therein.  The approach described here prunes along an intuitive path
of decreasing variance defined by the data and computed by the SVD.

\section{Efficient pruning using the truncated SVD}

Let $t$ be a given correlation threshold $0<t<1$, and let
$P\in\mathcal{R}^{n\times n}$ be a permutation that orders then entries of
$v_1$ in increasing order.  It follows from Equations \ref{cordist} and
\ref{vdist} that for any $i,j=1,2,\ldots,n$, if $\sigma_1^2 |v_{1i} - v_{1j}|^2
\ge 2(1-t)$ then the column vectors $a_j$ and $a_i$ do not meet the correlation
threshold.  Let $\ell$ be the longest run of successive points in $P v_1$
within the interval $\sqrt{2(1-t)}/\sigma_1$. For example, Figure \ref{fig1}
displays the entries of $v_1$ for the ``BicatYeast'' example from Section
\ref{examples} in increasing order.  The lines in the plot illustrate the
interval associated with the correlation threshold $t=0.99$. The quantity
$\ell$ is obtained by rolling the interval across all the points in the plot
and counting the maximum number of points that lie within the interval.  With
respect to the ordering $P$, $\ell$ represents the biggest index difference
that paris of indices corresponding to correlated vectors above the threshold
can exhibit.

\begin{figure}[!ht]
\label{fig1}
\begin{center}
<<eval=TRUE, echo=FALSE, fig=TRUE, height=5, width=7>>=
library(biclust)
data(BicatYeast)
A=t(BicatYeast)
s=svd(A)
i=order(s$v[,1])
plot(s$v[i,1], ylab=expression(P * v[1]), xlab="Sorted index", pch='.')
t=0.99
abline(h=c(-0.05,-0.05+sqrt(2*(1-t))/s$d[1]),col=2,lwd=0.5)
@
\caption{Example plot of the ordered entries of $v_1$ for the BicatYeast example from Section \ref{examples}. The red lines illustrate the interval corresponding threshold $t=0.99$. Points farther apart than this interval correspond to column vectors that do not meet the correlation threshold.}
\end{center}
\end{figure}

Let $D^i\in\mathcal{R}^{n-i, n}$, $i=1,2,\ldots,n-1$, be the $i$th order finite
difference matrix with $-1$ along the diagonal, $1$ along the $i$th
super-diagonal, and zeros elsewhere.  Consider just adjacent points of $P v_1$.
Then $D^1P v_1$ consists of the differences of adjacent entries in $P v_1$.
Entries such that $\sigma_1^2 (D^1 P v_1)^2 > 2(1-t)$
correspond to pairs of vectors that do not meet the correlation threshold,
(exponentiation is applied to the vector element-wise).
However, this observation is unlikely to rule out many pairs of vectors in
most problems. For example, the minimum distance between the points shown in
the example in Figure \ref{fig1} is less that $\sqrt{2(1-t)}/\sigma$, and no
pruning at all occurs.

Including more terms from Equation \ref{vdist} increases our ability to prune
pairs of vectors below the correlation threshold. For example, including
only one more term finds that 72\% of adjacent vectors fall below the
correlation threshold
in the example shown in Figure \ref{fig1} and are pruned by identifying
indices such that
\[
\sigma_1^2 (D^1 P v_1)^2 +
\sigma_2^2 (D^1 P v_2)^2 > 2(1-t),
\]
where exponentiation is applied element-wise to the vectors.
Note that we use the permutation $P$ defined by the order of vector $v_1$
throughout. Including five terms prunes over 97\% of adjacent vector pairs,
leaving only 11 candidate pairs of adjacent vectors.
In general, using $p\le k$ terms to prune pairs of vectors below the
correlation threshold boils down to evaluating the expression
\begin{equation}\label{proj}
(D^1 P V_{1:p} )^2 s_{1:p}^2 > 2(1-t),
\end{equation}
where 
$V_{1:p}$ denotes the first $p$ columns of the
matrix $V$, $s_{1:p}$ the vector of the first $p$ singular values,
and exponentiation is applied to vectors element-wise.

Pairs of vectors below the correlation threshold that lie more than one index
difference apart relative to the permutation $P$ can be similarly pruned by
replacing $D^1$ with $D^j$, $j=1,2,\ldots,\ell$ in the above expressions.
Following this procedure, we can produce a well-pruned candidate set of
possible vector pairs that meet a given correlation threshold using $\ell$
matrix vector products with matrices of order $n \times p$, where $p$ is chosen
in practice such that $p<<k$. The steps of this procedure are outlined in the
following algorithm:

\begin{algorithm}\label{prune}
Input: data matrix $A\in \mathcal{R}^{m\times n}$, correlation threshold $t$, maximum
truncated SVD rank $p<<k$.
\begin{enumerate}
\item Compute the truncated SVD $AV_{1:p} = U_{1:p}\mathrm{diag}(s_{1:p})$,
where $V_{1:p} = [v_1, v_2, \ldots, v_p]$
and $\mathrm{diag}(s_{1:p})$ denotes the diagonal matrix of the first $p$ singular values.
\item Compute permutation $P$ that orders points in $v_1$.
\item Compute $\ell$, the longest run of successive points in $P v_1$ within the interval $\sqrt{2(1-t)}/\sigma_1$.
\item Compute a set of candidate pairs $(\tilde{j},\tilde{j}+\tilde{i})$ with respect to the index order defined by the permutation $P$ that possibly meet the correlation threshold
\[
\bigcup_{\tilde{i}=1}^\ell
\left\{
(\tilde{j},\tilde{j}+\tilde{i}) : 
e_{\tilde{j}}^T(D^{\tilde{i}} P V_{1:p} )^2 s_{1:p}^2 \le 2(1-t)
\right\},
\]
where $e_{\tilde{j}}\in\mathcal{R}^n$ consists of $1$ in position $\tilde{j}$ and
zeros otherwise.
\item If there are judged to be  ``too many'' candidate pairs, increase $p$ and go to
step 1. Continue to increase $p$ in this way until the number of candidate
pairs stops decreasing (the best this algorithm can do).
\item Recover the non-permuted candidate column vector
indices $(j,i)$ from $j=e_{\tilde{j}}^T Pq$ and $i=e_{\tilde{j}+\tilde{i}}^TPq$ for every
pair $(\tilde{j},\tilde{j} + \tilde{i})$ in step 4, where
$q\in\mathcal{R}^n = [1,2,\ldots,n]^T$.
\item Compute the full correlation coefficient for each column vector pair identified in
the previous step, applying the correlation threshold to this reduced set of values.
\end{enumerate}
\end{algorithm}
Because the singular values $\sigma_j$, $j=1,2,\ldots,n$ are non-increasing and
decrease in proportion to variability in the data, it's often reasonable to
judge whether there are ``too many'' candidate pairs (step 5) after only a 1-d
projection $\tilde{i}=1$ in step 4 of the algorithm.

Algorithm \ref{prune} by construction guarantees that no pruned vector pair
meets the given correlation threshold; that is, the pruning can not produce any
``false negatives.''  The algorithm typically prunes the vast majority of pairs
of vectors below the correlation threshold with truncated singular value
decompositions of only relatively low rank $p$.  The choice of $p$ is a
balance between work in computing a truncated SVD and work required by the
$\ell$ matrix vector products of order $n\times p$ in step 4.

\section{Fast truncated SVD for general matrices}\label{irlba}

We have so far assumed that the mean of each column vector of $A$ is zero and
the columns are scaled to have unit norm, but we really want a method that works
with general matrices $A$. We've also implicitly assumed that we can
efficiently compute a relatively low-rank truncated SVD of potentially
large  matrices required by step 1 of Algorithm \ref{prune}.  Fortunately,
both needs are met by one algorithm.

The augmented, implicitly restarted Lanczos bidiagonalization algorithm (IRLBA)
of Baglama and Reichel \cite{br} efficiently computes truncated singular value
decompositions of large dense or sparse matrices. Reference implementations are
available for R\cite{irlba-R}, Matlab\cite{irlba-M}, and Python\cite{irlba-P}.
The R implementation includes efficient methods for re-starting the algorithm
to increase the subspace dimension (see step 5 of Algorithm \ref{prune}), and
efficient methods for computing the SVD of scaled and centered matrices without
explicitly forming a new matrix.

XXX scaling/centering discussion and example

\section{Numerical experiments}\label{examples}

%%<<eval=TRUE,echo=FALSE>>=
%%library(irlba)
%%
%%print("R code here")
%%@


\section{Conclusion}\label{conclusion}


%% XXX switch to bibtex

\begin{thebibliography}{99}
\bibitem{br}
Baglama, J. and Reichel, L., Augmented Implicitly Restarted Lanczos Bidiagonalization Methods, SIAM J. Sci. Comput. 2005.
\bibitem{mnl}
Mueen, A., Nath, S., and Liu, J., Fast approximate correlation for
massive time-series data, in Proceedings of the 2010 ACM SIGMOD International
Conference on Management of data, 2010, pp. 171-182.
\bibitem{raf}
Rafiei, D., On similarity-based queries for time series data,
in Data Engineering Proceedings., IEEE, 1999.
\bibitem{zf}
Zhang, J. and Feigenbaum, J, Finding Highly Correlated Pairs Efficiently with Powerful Pruning,
in Proceedings of ACM Conference on Information and Knowledge Management (CIKM), 2006, pp. 152-161.
\bibitem{irlba-R}
\url{https://cran.r-project.org/web/packages/irlba/index.html}
\bibitem{irlba-P}
\url{https://github.com/bwlewis/irlbpy}
\bibitem{irlba-M}
\url{http://www.math.uri.edu/~jbaglama/software/irlba.m}
\end{thebibliography}
\end{document}
