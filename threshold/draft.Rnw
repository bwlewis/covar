%%
%% Instructions for processing this document into a PDF document
%%
%% FIRST NOTE, you need the 'biclust' package installed in R.
%% Some of the examples will come from data sets in this package.
%% You can install it from the R console with, for example:
%%
%% install.packages("biclust",repos="http://cran.case.edu")
%%
%% Now you can prepare the PDF of the paper with any of:
%%
%% From a shell:
%% R CMD Sweave --pdf draft.Rnw
%%
%% From an R prompt:
%% Sweave("draft.Rnw")
%% tools::texi2pdf("draft.tex")
%%
%% From RStudio:
%% click on the little "Compile PDF" button
%%
%% Note that you should probably run the above a few times because
%% LaTeX generally takes a few runs to get reference numbers right.
%%
\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcounter{algorithmctr}
\newenvironment{algorithm}{
   \refstepcounter{algorithmctr}
   \bigskip\noindent
   \textbf{Algorithm \thealgorithmctr\\}
}
{\bigskip}
\numberwithin{algorithmctr}{section}


\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}



\title{Efficient Thresholded Correlation using Truncated Singular Value Decomposition}
\author{
James Baglama\\
Department of Mathematics,\\University of Rhode Island
\and
Bryan Lewis\\
Paradigm4, Inc.\\
\and
Alex Poliakov\\
Paradigm4, Inc.
}

\date{}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

\begin{abstract}

Efficiently computing a subset of a correlation matrix consisting of values
above a specified threshold is important to many practical applications.
Real-world problems in genomics, machine learning, financial and other time
series and other applications can produce correlation matrices
too large to explicitly form and tractably compute. Often, only values
corresponding to highly-correlated vectors are of interest, and those values
typically make up a small fraction of the overall correlation matrix. We
present a method based on the singular value decomposition (SVD) and its
relationship to the data covariance structure that can efficiently compute
thresholded subsets of very large correlation matrices.

\end{abstract}


\section{Introduction}\label{intro}

Finding highly-correlated pairs among a large set of vectors is an important
part of many applications. For instance, subsets of highly-correlated vectors
of gene expression values may be used in the discovery of networks of genes
relevant to particular biological processes\cite{genomics}.
Identification of highly-correlated pairs may be used in feature selection
algorithms for machine learning applications\cite{ml1, ml2}. It's also
important to many time series and image processing applications, see for
example\cite{timeseries, svd-similarity} and references therein.

The number of correlation coefficients grows quadratically with the number of
vectors. Simply computing all pairs of correlation coefficients and then
filtering out coefficients below a given threshold may not be computationally
feasable for high-dimensional data arising in modern genomics and other
applications. Considerable attention has been devoted to methods called
\emph{pruning methods} that cheaply prune away all but the most
highly-correlated pairs of vectors.  A variety of pruning methods have been
described for the thresholded correlation problem; see, for
example\cite{prune1, prune2} and the references therein. Related methods
approximate a set containing the most highly-correlated pairs\cite{prune3,
timeseries}. A method of Wu, et al.\cite{svd-similarity}, conceptually very
similar to our approach, uses the singular value decomposition (SVD) to find
pairs of close vectors with respect to a distance metric (instead of
correlation).  Our method described below prunes along an intuitive path of
decreasing variance defined by the data and computed by the SVD.


Consider a real-valued data matrix $A$ consisting of $m$ observations of $n$
column vectors.  Denote the columns of $A$ as $a_j\in\mathcal{R}^m$ for
$j=1,2,\ldots,n$, $A=[a_1, a_2, \ldots, a_n]\in\mathcal{R}^{m\times n}$, and
let $k=\mathrm{rank}(A)$.  Assume that the mean of each column $a_j$ is zero
and that each column has unit norm $\|a_j\| = 1$. Here and below, $\|\cdot\|$
denotes the Euclidean vector norm. Then the Pearson sample correlation matrix
$\mathrm{cor}(A)=A^TA$.  Note that under these assumptions the sample
correlation and covariance matrices are the same.  Section \ref{irlba} below
illustrates how to relax the unit norm and zero mean assumptions in practice.

Let $A=U\Sigma V^T$ be the singular value decomposition of $A$, where
$U\in\mathcal{R}^{m\times k}$, $V\in\mathcal{R}^{n\times k}$,
$U^TU = V^TV = I$,
and $\Sigma\in\mathcal{R}^{k\times k}$ is a diagonal matrix with
diagonal entries $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_k > 0$.  For
later convenience, we write $s_{1:p}=[\sigma_1, \sigma_2, \ldots, \sigma_p]^T$
to be the vector of the first $p\le k$ singular values along the diagonal of
$\Sigma$.  Denote the columns of $V$ as $V=[v_1, v_2, \ldots, v_k]$, where each
$v_j\in\mathcal{R}^n$, $j=1,2,\ldots,k$.  Then $\mathrm{cor}(A) = A^TA =
V\Sigma^2 V^T$ is a symmetric eigenvalue decomposition of the correlation
matrix with nonzero eigenvalues $\sigma_1^2, \sigma_2^2, \ldots, \sigma_k^2$.
The span of the columns of $V$ form an orthonormal basis of the range of the
correlation matrix $A^TA$.

Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix $A$
(including possibly the case $i=j$). Denote the correlation of these two
vectors by $\mathrm{cor}(a_i,a_j) = a_j^T a_i$.  The following simple lemma
establishes a relationship between correlation and Euclidean distance between
two vectors.

\begin{lemma}
Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix
$A$. Then
\begin{equation}\label{cordist}
\mathrm{cor}(a_i,a_j) = a_i^Ta_j = 1 - \|a_i - a_j\|^2/2.
\end{equation}
In particular, for a given correlation threshold $t$, $0<t<1$, if
$\|a_i - a_j\|^2 > 2(1-t)$ then
$\mathrm{cor}(a_i, a_j) < t$.
\end{lemma}
%%\begin{proof}
%%\begin{eqnarray*}
%%\|a_i - a_j\|^2 &=& (a_i - a_j)^T(a_i - a_j) \\
%%&=& a_i^T a_i + a_j^T a_j - 2a_i^T a_j \\
%%&=& \|a_i\|^2 + \|a_j\|^2 - 2a_i^T a_j \\
%%&=& 2 - 2a_i^T a_j.
%%\end{eqnarray*}
%%The lemma follows by solving for $a_i^Ta_j$.
%%\end{proof}

Equation \ref{cordist} equates the problem of finding pairs of highly
correlated column vectors from the matrix $A$ with a problem of finding paris
of vectors sufficiently close together. For example, correlation values larger
than 0.99 may only be associated with column vectors from the matrix $A$ whose
Euclidean distance is less than $\sqrt{0.02}$.  Vectors farther apart can not
meet the correlation threshold.

The following lemma expresses the Euclidean distance between any two columns of
the matrix $A$ as a weighted sum of projected interval distances along the SVD
basis vectors forming the columns of $V$.
\begin{lemma}
Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix
$A$. Then
\begin{equation}\label{vdist}
\|a_i - a_j\|^2 =
\sigma_1^2 |e_i^Tv_{1} - e_j^Tv_{1}|^2 + 
\sigma_2^2 |e_i^Tv_{2} - e_j^Tv_{2}|^2 + \cdots + 
\sigma_k^2 |e_i^Tv_{ki} - e_j^Tv_{k}|^2,
\end{equation}
\end{lemma}
where here and below, $e_{j}\in\mathcal{R}^n$ represents the $j$th unit basis
vector consisting of $1$ in position $j$ and zeros otherwise.
(Thus, $e_j^Tv_{k}$ is the $j$th position in the vector $v_k$; that is, the
$[j,k]$ entry in the matrix $V$.)

%%\begin{proof}
%%\begin{align*}
%%\|a_i - a_j\|^2 &= (a_i - a_j)^T(a_i - a_j) \\
%%&= a_i^Ta_i + a_j^T a_j - 2a_i^Ta_j \\
%%&= \sigma_1^2 v_{1i}^2 + \sigma_2^2 v_{2i}^2 + \cdots + \sigma_k^2 v_{ki}^2 \\
%%&\phantom{=} + \sigma_1^2 v_{1j}^2 + \sigma_2^2 v_{2j}^2 + \cdots + \sigma_k^2 v_{kj}^2 \\
%%&\phantom{=} -2(\sigma_1^2 v_{1i}v_{1j} + \sigma_2^2 v_{2i}v_{2j} + \cdots + \sigma_k^2 v_{ki}v_{kj}) \\
%%&= \sigma_1^2 (v_{1i}^2 + v_{1j}^2 - 2v_{1i}v_{1j}) \\
%%&\phantom{=} + \sigma_2^2 (v_{2i}^2 + v_{2j}^2 - 2v_{2i}v_{2j}) \\
%%&\phantom{=} + \cdots \\
%%&\phantom{=} + \sigma_k^2 (v_{ki}^2 + v_{kj}^2 - 2v_{ki}v_{kj}) \\
%%&= \sigma_1^2 |v_{1i} - v_{1j}|^2
%% + \sigma_2^2 |v_{2i} - v_{2j}|^2
%% + \cdots
%% + \sigma_k^2 |v_{ki} - v_{kj}|^2.
%%\end{align*}
%%\end{proof}

Note that each term in the sum is nonnegative and the weights $\sigma_j^2$,
$j=1,2,\ldots,k$ are nonincreasing and defined by the covariance structure of
the data.  Given a correlation threshold $t$, $0<t<1$, Equations \ref{cordist}
and \ref{vdist} together suggest that pairs of vectors can be excluded from
consideration whenever their projected distance is too large.  For example if
$\sigma_1^2 |e_i^Tv_{1} - e_j^Tv_{1}|^2 \ge 2(1-t)$, then we can conclude from
just two scalar values that the column vectors $a_i$ and $a_j$ do not meet the
correlation threshold $t$.
In practice, many pairs of vectors may be pruned in this fashion by
inspecting distances in low-dimensional subspaces, substantially reducing the
complexity of computing thresholded correlation matrices.

\section{Efficient pruning using the truncated SVD}

Let $t$ be a given correlation threshold $0<t<1$, and let
$P\in\mathcal{R}^{n\times n}$ be a permutation matrix that orders then entries
of $v_1$ in increasing order.  It follows from Equations \ref{cordist} and
\ref{vdist} that for any $i,j=1,2,\ldots,n$, if $\sigma_1^2 |e_i^Tv_{1} -
e_j^Tv_{1j}|^2 \ge 2(1-t)$ then $\mathrm{cor}(a_i,a_j) < t$.  Let $\ell$ be the
longest run of successive points in $P v_1$ within the interval
$\sqrt{2(1-t)}/\sigma_1$. For example, Figure \ref{fig1} displays the entries
of $v_1$ for the small ``BicatYeast'' example (where $A\in\mathcal{R}^{70\times
419}$) from Section \ref{examples} in increasing order.  The lines in the plot
illustrate the interval associated with the correlation threshold $t=0.95$. The
quantity $\ell$ is obtained by rolling the interval across all the points in
the plot and counting the maximum number of points that lie within the
interval.  With respect to the ordering $P$, $\ell$ represents the biggest
index difference that paris of indices corresponding to correlated vectors
above the threshold can exhibit.

\begin{figure}[!ht]
\begin{center}
<<eval=TRUE, echo=FALSE, fig=TRUE, height=5, width=7>>=
library(biclust)
data(BicatYeast)
A=t(BicatYeast)
s=svd(A)
i=order(s$v[,1])
plot(s$v[i,1], ylab=expression(P * v[1]), xlab="Sorted index", pch='.')
t=0.95
abline(h=c(-0.05,-0.05+sqrt(2*(1-t))/s$d[1]),col=2,lwd=0.5)
@
\caption{
Example plot of the ordered entries of $v_1$ for the BicatYeast example from
Section \ref{examples}. The red lines illustrate the interval corresponding
threshold $t=0.95$ placed at an arbitrary location on the vertical axis. Points
farther apart than this interval correspond to column vectors that do not meet
the correlation threshold.
\label{fig1}
}
\end{center}
\end{figure}

Let $D^i\in\mathcal{R}^{n-i, n}$, $i=1,2,\ldots,n-1$, be the $i$th order finite
difference matrix with $-1$ along the diagonal, $1$ along the $i$th
super-diagonal, and zeros elsewhere.  Consider just adjacent points of $P v_1$.
Then $D^1P v_1$ consists of the differences of adjacent entries in $P v_1$.
Entries such that $\sigma_1^2 (D^1 P v_1)^2 > 2(1-t)$ correspond to pairs of
vectors that do not meet the correlation threshold, where here and below
an exponent applied to a vector denotes element-wise exponentiation.

Even a 1-d projected interval distance threshold may prune many possible
non-adjacent vector pairs with respect to the ordering defined by $P$ as
illustrated by the example in Figure \ref{fig1}.  However, this observation is
unlikely to rule out many adjacent pairs of vectors in most problems. For
example, the minimum distance between the points shown in the example in Figure
\ref{fig1} is less that $\sqrt{2(1-t)}/\sigma_1$, and no pruning at all occurs.

Including more terms from Equation \ref{vdist} increases our ability to prune
adjacent pairs of vectors with respect to the ordering defined by $P$
below the correlation threshold. For example,
including only one more term finds that 49\% of adjacent vectors fall below the
a correlation threshold of $0.95$ in the example shown in Figure \ref{fig1} and
are pruned by identifying indices such that
\[
\sigma_1^2 (D^1 P v_1)^2 +
\sigma_2^2 (D^1 P v_2)^2 > 2(1-t).
\]
Note that we use the permutation $P$ defined by the order of vector $v_1$
throughout. Including ten terms prunes over 96\% of adjacent vector pairs,
leaving only 15 candidate pairs of adjacent vectors that might possibly
meet the correlation threshold.
In general, using $p\le k$ terms to prune pairs of vectors below the
correlation threshold boils down to evaluating the expression
\begin{equation}\label{proj}
(D^1 P V_{1:p} )^2 s_{1:p}^2 > 2(1-t),
\end{equation}
where 
$V_{1:p}$ denotes the first $p$ columns of the
matrix $V$, $s_{1:p}$ the vector of the first $p$ singular values.

Pairs of vectors below the correlation threshold that lie more than one index
difference apart relative to the permutation $P$ can be similarly pruned by
replacing $D^1$ with $D^j$, $j=2,3,\ldots,\ell$ in the above expressions.
Following this process, we can produce a well-pruned candidate set of possible
vector pairs that meet a given correlation threshold using $\ell$ matrix vector
products with matrices of order $n \times p$, where $p$ is chosen in practice
such that $p<<k$. Setting $p=10$ for the example shown in Figure
\ref{fig1} prunes all but $431$ possible vector pairs out of a total
$419(419-1)/2 = 87,571$ with $\ell=138$.

These steps are formalized in Algorithm \ref{prune} below. The algorithm
proceeds in two main parts: steps 1--6 prune pairs of vectors down to a small
candidate set of vector pairs that may meet the correlation threshold; step 7
computes the correlation values and applies the threshold across the reduced
set of vector pairs.

\begin{algorithm}\label{prune}
Input: data matrix $A\in \mathcal{R}^{m\times n}$ with columns scaled to have
zero mean and unit norm, correlation threshold $t$,
truncated SVD rank $p<<k$.
\begin{enumerate}
\item Compute the truncated SVD $AV_{1:p} = U_{1:p}\mathrm{diag}(s_{1:p})$,
where\\
$V_{1:p}=[v_1, v_2, \ldots, v_p]$
and $\mathrm{diag}(s_{1:p})$ denotes the diagonal matrix of the first $p$ singular values.
\item Compute permutation $P$ that orders points in $v_1$.
\item Compute $\ell$, the longest run of successive points in $P v_1$ within the interval $\sqrt{2(1-t)}/\sigma_1$.
\item Compute a set of candidate index pairs $(\tilde{j},\tilde{j}+\tilde{i})$ with respect to the index order defined by the permutation $P$ that possibly meet the correlation threshold
\[
\bigcup_{\tilde{i}=1}^\ell
\left\{
(\tilde{j},\tilde{j}+\tilde{i}) : 
e_{\tilde{j}}^T(D^{\tilde{i}} P V_{1:p} )^2 s_{1:p}^2 \le 2(1-t)
\right\}.
\]
\item If there are judged to be  ``too many'' candidate pairs, increase $p$ and go to
step 1. Continue to increase $p$ in this way until the number of candidate
pairs is sufficiently small or stops decreasing (the best this algorithm can do).
\item Recover the non-permuted candidate column vector
indices $(j,i)$ from $j=e_{\tilde{j}}^T Pq$ and $i=e_{\tilde{j}+\tilde{i}}^TPq$ for every
pair $(\tilde{j},\tilde{j} + \tilde{i})$ in step 4, where
$q\in\mathcal{R}^n = [1,2,\ldots,n]^T$.
\item Compute the full correlation coefficient for each column vector pair identified in
the previous step, applying the correlation threshold to this reduced set of values.
\end{enumerate}
\end{algorithm}
Because the singular values $\sigma_j$, $j=1,2,\ldots,n$ are non-increasing and
decrease in proportion to variability in the data, it's often reasonable to
judge whether there are ``too many'' candidate pairs (step 5) after only a 1-d
projection $\tilde{i}=1$ in step 4 of the algorithm.

Algorithm \ref{prune} by construction guarantees that no pruned vector pair
exceeds the given correlation threshold; that is, the pruning can not produce
any false negatives.  The algorithm typically prunes the vast majority of pairs
of vectors below the correlation threshold with truncated singular value
decompositions of only relatively low rank $p$.  The choice of $p$ is a balance
between work in computing a truncated SVD and work required by the $\ell$
matrix vector products of order $n\times p$ in step 4.



\section{General matrices and fast truncated SVD}\label{irlba}

We have so far assumed that the mean of each column vector of the $m\times n$
matrix $A$ is zero and the columns are scaled to have unit norm, but we really
want a method that works with general matrices $A$.  We also want a way to
efficiently compute a relatively low-rank truncated SVD of potentially large
matrices required by step 1 of Algorithm \ref{prune}.  Fortunately, both
desires are met by one algorithm.

The augmented implicitly restarted Lanczos bidiagonalization algorithm (IRLBA)
of Baglama and Reichel \cite{irlba} efficiently computes truncated singular value
decompositions of large dense or sparse matrices. Reference implementations are
available for R\cite{irlbar}, Matlab\cite{irlbam}, and Python\cite{irlbap}.

We relax the column mean and scale assumptions without introducing much additional
computational or storage overhead as follows.  Let
$z=[z_1,z_2,\ldots,z_n]\in\mathcal{R}^n$ represent the vector of column means
of the matrix $A$, $w=[w_1^2,w_2^2,\ldots,w_n^2]\in\mathcal{R}^n$ be the vector
of squared column norms of $A$, and $W\in\mathcal{R}^{n\times n}$ a diagonal
matrix with diagonal entries $1/\sqrt{w_i^2 - m z_i^2}$.  Let
$e\in\mathcal{R}^m$ be a vector of all ones.  Then $(A - ez^T)W$ is a centered
matrix with zero column means and scaled to have unit column norms, and
\[
\mathrm{cor}(A) = W^T (A-ez^T)^T (A-ez^T) W.
\]
The IRLBA, based on the Lanczos process, is a method of iterated matrix-vector
products. The main idea behind efficient application of IRLB to correlation
problems replaces matrix-vector products of the form $Ax$ with $AWx - ez^TWx$ in
the iterations, implicitly working with a scaled and centered matrix without
forming it.  This comes at the cost of storing two additional length $n$ vectors and
at most computing two additional vector inner products per matrix-vector product.



\section{Numerical experiments}\label{examples}

%%<<eval=TRUE,echo=FALSE>>=
%%library(irlba)
%%
%%print("R code here")
%%@


\section{Conclusion}\label{conclusion}



\begin{thebibliography}{99}

%% Related use of SVD in other methods
\bibitem{spca}
Bair, Eric, et al. Prediction by supervised principal components. Journal of the American Statistical Association 101.473 (2006).

%% irlba
\bibitem{irlba}
Baglama, James, and Lothar Reichel. Augmented implicitly restarted Lanczos bidiagonalization methods. SIAM Journal on Scientific Computing 27.1 (2005): 19-42.
\bibitem{irlbam}
Baglama, James,  {\url{http://www.math.uri.edu/~jbaglama/software/irlba.m}}.

%% Applications in machine learning
\bibitem{ml1}
Hall, Mark A. Correlation-based feature selection for machine learning. Diss. The University of Waikato, 1999.

%% IRLBA implementations
\bibitem{irlbap}
Kane, Michael and Lewis, Bryan,  \url{https://github.com/bwlewis/irlbpy}.

%% IRLBA implementations
\bibitem{irlbar}
Lewis, Bryan,  \url{https://cran.r-project.org/web/packages/irlba}.

%% Approximate correlation
\bibitem{timeseries}
Mueen, Abdullah, Suman Nath, and Jie Liu. Fast approximate correlation for massive time-series data. Proceedings of the 2010 ACM SIGMOD International Conference on Management of data. ACM, 2010.

%% Applications in genomics
\bibitem{genomics}
Song, Lin, Peter Langfelder, and Steve Horvath. Comparison of co-expression measures: mutual information, correlation, and model based indices. BMC bioinformatics 13.1 (2012): 328.

%% SVD similarity -- extremely similar to the approach in this paper!
%% Applied to images
\bibitem{svd-similarity}
Wu, Daniel, et al. Efficient retrieval for browsing large image databases. Proceedings of the fifth international conference on Information and knowledge management. ACM, 1996.

%% Other correlation pruning methods
\bibitem{prune1}
Xiong, Hui, et al. TAPER: A two-step approach for all-strong-pairs correlation query in large databases. Knowledge and Data Engineering, IEEE Transactions on 18.4 (2006): 493-508.
\bibitem{prune2}
Xiong, Hui, Mark Brodie, and Sheng Ma. Top-cop: Mining top-k strongly correlated pairs in large databases. Data Mining, 2006. ICDM'06. Sixth International Conference on. IEEE, 2006.

%% Applications in machine learning
\bibitem{ml2}
Yu, Lei, and Huan Liu. Feature selection for high-dimensional data: A fast correlation-based filter solution. ICML. Vol. 3. 2003.

%% Other correlation pruning methods
\bibitem{prune3}
Zhang, Jian, and Joan Feigenbaum. Finding highly correlated pairs efficiently with powerful pruning. Proceedings of the 15th ACM international conference on Information and knowledge management. ACM, 2006.

\end{thebibliography}
\end{document}
