%%
%% Instructions for processing this document into a PDF document
%%
%% FIRST NOTE, you need the 'biclust' package installed in R.
%% Some of the examples will come from data sets in this package.
%% You can install it from the R console with, for example:
%%
%% install.packages("biclust",repos="http://cran.case.edu")
%%
%% Now you can prepare the PDF of the paper with any of:
%%
%% From a shell:
%% R CMD Sweave --pdf draft.Rnw
%%
%% From an R prompt:
%% Sweave("draft.Rnw")
%% tools::texi2pdf("draft.tex")
%%
%% From RStudio:
%% click on the little "Compile PDF" button
%%
%% Note that you should probably run the above a few times because
%% LaTeX generally takes a few runs to get reference numbers right.
%%
\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{hyperref}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcounter{algorithmctr}
\newenvironment{algorithm}{
   \refstepcounter{algorithmctr}
   \bigskip\noindent
   \textbf{Algorithm \thealgorithmctr\\}
}
{\bigskip}
\numberwithin{algorithmctr}{section}


\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}




\title{Efficient Thresholded Correlation using Truncated Singular Value Decomposition}
\author{Some dudes}

\date{}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

\begin{abstract}

Efficiently computing a subset of a correlation matrix consisting of values
above a specified threshold is important to many practical applications.
Real-world problems in genomics, machine learning, finance, and other fields
can produce correlation matrices too large to explicitly form and tracticably
compute. Often, only values corresponding to highly-correlated vectors are of
interest, and those values typically make up a small fraction of the overall
correlation matrix. We present a method based on the singular value
decomposition (SVD) and its relationship to the data covariance structure that
can efficiently compute thresholded subsets of very large correlation matrices.

\end{abstract}



\section{Introduction}\label{intro}

XXX add discussion of applications (genomics?) and previous work (especially
\cite{xstk1},\cite{xstk2} and
related but different work that use PCA: \cite{latj} and \cite{bhpt} are related but different),
then on to the preliminaries:
\[\]

Consider a real-valued data matrix $A$ consisting of $m$ observations of $n$
column vectors.  Denote the columns of $A$ as $a_j\in\mathcal{R}^m$ for
$j=1,2,\ldots,n$, $A=[a_1, a_2, \ldots, a_n]\in\mathcal{R}^{m\times n}$, and
let $k=\mathrm{rank}(A)$.  Assume that the mean of each column $a_j$ is zero
and that each column has unit norm $\|a_j\| = 1$. Here and below, $\|\cdot\|$
denotes the Euclidean vector norm. Then the sample correlation matrix
$\mathrm{cor}(A)=A^TA$.  Note that under these assumptions the sample
correlation and covariance matrices are the same.  Section \ref{irlba} below
illustrates how to relax the unit norm and zero mean assumptions in practice.

Let $A=U\Sigma V^T$ be the singular value decomposition of $A$, where
$U\in\mathcal{R}^{m\times k}$, $U^TU = I$, $V\in\mathcal{R}^{n\times k}$,
$V^TV=I$, and $\Sigma\in\mathcal{R}^{k\times k}$ is a diagonal matrix with
diagonal entries $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_k > 0$.  For
later convenience, we write $s_{1:p}=[\sigma_1, \sigma_2, \ldots, \sigma_p]^T$
to be the vector of the first $p\le k$ singular values along the diagonal of
$\Sigma$.  Denote the columns of $V$ as $V=[v_1, v_2, \ldots, v_k]$, where each
$v_j\in\mathcal{R}^n$, $j=1,2,\ldots,k$.  Then $\mathrm{cor}(A) = A^TA =
V\Sigma^2 V^T$ is a symmetric eigenvalue decomposition of the correlation
matrix with nonzero eigenvalues $\sigma_1^2, \sigma_2^2, \ldots, \sigma_k^2$.
The span of the columns of $V$ form an orthonormal basis of the range of the
correlation matrix $A^TA$.

Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix $A$
(including possibly the case $i=j$). Denote the correlation of these two
vectors by $\mathrm{cor}(a_i,a_j) = a_j^T a_i$.  The following simple lemma
establishes a relationship between correlation and Euclidean distance between
two vectors.

\begin{lemma}
Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix
$A$. Then
\begin{equation}\label{cordist}
\mathrm{cor}(a_i,a_j) = a_i^Ta_j = 1 - \|a_i - a_j\|^2/2.
\end{equation}
In particular, for a given correlation threshold $t$, $0<t<1$, pairs
of vectors $a_i$ and $a_j$ such that $\|a_i - a_j\|^2 > 2(1-t)$ lie
below the correlation threshold.
\end{lemma}
\begin{proof} (XXX probably omit this)
\begin{eqnarray*}
\|a_i - a_j\|^2 &=& (a_i - a_j)^T(a_i - a_j) \\
&=& a_i^T a_i + a_j^T a_j - 2a_i^T a_j \\
&=& \|a_i\|^2 + \|a_j\|^2 - 2a_i^T a_j \\
&=& 2 - 2a_i^T a_j.
\end{eqnarray*}
The lemma follows by solving for $a_i^Ta_j$.
\end{proof}

Equation \ref{cordist} equates the problem of finding pairs of highly
correlated column vectors from the matrix $A$ with a problem of finding paris
of vectors sufficiently close together. For example, correlation values larger
than 0.99 may only be associated with column vectors from the matrix $A$ whose
Euclidean distance is less than $\sqrt{0.02}$.  Vectors farther apart will not
meet the correlation threshold.

The following lemma expresses the Euclidean distance between any two columns of the
matrix $A$ as a weighted sum of projected interval distances along the SVD
basis vectors forming the columns of $V$.
\begin{lemma}
Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix
$A$. Then
\begin{equation}\label{vdist}
\|a_i - a_j\|^2 =
\sigma_1^2 |v_{1i} - v_{1j}|^2 + 
\sigma_2^2 |v_{2i} - v_{2j}|^2 + \cdots + 
\sigma_k^2 |v_{ki} - v_{kj}|^2,
\end{equation}
where $v_{kj}$ denotes the $jth$ position in the vector $v_k$ (that is, the
$[j,k]$ entry in the matrix $V$).
\end{lemma}

%%\begin{proof}
%%\begin{align*}
%%\|a_i - a_j\|^2 &= (a_i - a_j)^T(a_i - a_j) \\
%%&= a_i^Ta_i + a_j^T a_j - 2a_i^Ta_j \\
%%&= \sigma_1^2 v_{1i}^2 + \sigma_2^2 v_{2i}^2 + \cdots + \sigma_k^2 v_{ki}^2 \\
%%&\phantom{=} + \sigma_1^2 v_{1j}^2 + \sigma_2^2 v_{2j}^2 + \cdots + \sigma_k^2 v_{kj}^2 \\
%%&\phantom{=} -2(\sigma_1^2 v_{1i}v_{1j} + \sigma_2^2 v_{2i}v_{2j} + \cdots + \sigma_k^2 v_{ki}v_{kj}) \\
%%&= \sigma_1^2 (v_{1i}^2 + v_{1j}^2 - 2v_{1i}v_{1j}) \\
%%&\phantom{=} + \sigma_2^2 (v_{2i}^2 + v_{2j}^2 - 2v_{2i}v_{2j}) \\
%%&\phantom{=} + \cdots \\
%%&\phantom{=} + \sigma_k^2 (v_{ki}^2 + v_{kj}^2 - 2v_{ki}v_{kj}) \\
%%&= \sigma_1^2 |v_{1i} - v_{1j}|^2
%% + \sigma_2^2 |v_{2i} - v_{2j}|^2
%% + \cdots
%% + \sigma_k^2 |v_{ki} - v_{kj}|^2.
%%\end{align*}
%%\end{proof}

Note that each term in the sum is nonnegative and the weights $\sigma_j^2$,
$j=1,2,\ldots,k$ are nonincreasing and defined by the covariance structure of
the data.  Given a correlation threshold $t$, $0<t<1$, Equations \ref{cordist}
and \ref{vdist} together suggest that pairs of vectors can be excluded from
consideration whenever their projected distance is too large.  For example if
$\sigma_1^2 |v_{1i} - v_{1j}|^2 \ge 2(1-t)$, then we can conclude from just two
scalar values that the column vectors $a_i$ and $a_j$ do not meet the
correlation threshold $t$.

In practice, many pairs of vectors may be ruled out in this fashion by
inspecting distances in low-dimensional subspaces, substantially reducing the
complexity of computing thresholded correlation matrices.  Approaches that
cheaply omit pairs of vectors from consideration are called \emph{pruning
methods}, and a variety of pruning methods have been described elsewhere for
the thresholded correlation problem; see, for example\cite{xstk1,xstk2,zf,mnl}
and the references therein.  The approach described here prunes along an
intuitive path of decreasing variance defined by the data and computed by the
SVD.

\section{Efficient pruning using the truncated SVD}

Let $t$ be a given correlation threshold $0<t<1$, and let
$P\in\mathcal{R}^{n\times n}$ be a permutation matrix that orders then entries
of $v_1$ in increasing order.  It follows from Equations \ref{cordist} and
\ref{vdist} that for any $i,j=1,2,\ldots,n$, if $\sigma_1^2 |v_{1i} - v_{1j}|^2
\ge 2(1-t)$ then the column vectors $a_j$ and $a_i$ do not meet the correlation
threshold.  Let $\ell$ be the longest run of successive points in $P v_1$
within the interval $\sqrt{2(1-t)}/\sigma_1$. For example, Figure \ref{fig1}
displays the entries of $v_1$ for the ``BicatYeast'' example from Section
\ref{examples} in increasing order.  The lines in the plot illustrate the
interval associated with the correlation threshold $t=0.99$. The quantity
$\ell$ is obtained by rolling the interval across all the points in the plot
and counting the maximum number of points that lie within the interval.  With
respect to the ordering $P$, $\ell$ represents the biggest index difference
that paris of indices corresponding to correlated vectors above the threshold
can exhibit.

\begin{figure}[!ht]
\begin{center}
<<eval=TRUE, echo=FALSE, fig=TRUE, height=5, width=7>>=
library(biclust)
data(BicatYeast)
A=t(BicatYeast)
s=svd(A)
i=order(s$v[,1])
plot(s$v[i,1], ylab=expression(P * v[1]), xlab="Sorted index", pch='.')
t=0.99
abline(h=c(-0.05,-0.05+sqrt(2*(1-t))/s$d[1]),col=2,lwd=0.5)
@
\caption{
Example plot of the ordered entries of $v_1$ for the BicatYeast example from Section \ref{examples}. The red lines illustrate the interval corresponding threshold $t=0.99$. Points farther apart than this interval correspond to column vectors that do not meet the correlation threshold.
\label{fig1}
}
\end{center}
\end{figure}

Let $D^i\in\mathcal{R}^{n-i, n}$, $i=1,2,\ldots,n-1$, be the $i$th order finite
difference matrix with $-1$ along the diagonal, $1$ along the $i$th
super-diagonal, and zeros elsewhere.  Consider just adjacent points of $P v_1$.
Then $D^1P v_1$ consists of the differences of adjacent entries in $P v_1$.
Entries such that $\sigma_1^2 (D^1 P v_1)^2 > 2(1-t)$ correspond to pairs of
vectors that do not meet the correlation threshold, where here and below
an exponent applied to a vector denotes element-wise exponentiation.
However, this observation is unlikely to rule out many pairs of vectors in most
problems. For example, the minimum distance between the points shown in the
example in Figure \ref{fig1} is less that $\sqrt{2(1-t)}/\sigma_1$, and no
pruning at all occurs.

Including more terms from Equation \ref{vdist} increases our ability to prune
pairs of vectors below the correlation threshold. For example, including only
one more term finds that 72\% of adjacent vectors fall below the a correlation
threshold of $0.99$ in the example shown in Figure \ref{fig1} and are pruned by
identifying indices such that
\[
\sigma_1^2 (D^1 P v_1)^2 +
\sigma_2^2 (D^1 P v_2)^2 > 2(1-t).
\]
Note that we use the permutation $P$ defined by the order of vector $v_1$
throughout. Including five terms prunes over 97\% of adjacent vector pairs,
leaving only 11 candidate pairs of adjacent vectors that might possibly
meet the correlation threshold.
In general, using $p\le k$ terms to prune pairs of vectors below the
correlation threshold boils down to evaluating the expression
\begin{equation}\label{proj}
(D^1 P V_{1:p} )^2 s_{1:p}^2 > 2(1-t),
\end{equation}
where 
$V_{1:p}$ denotes the first $p$ columns of the
matrix $V$, $s_{1:p}$ the vector of the first $p$ singular values.

Pairs of vectors below the correlation threshold that lie more than one index
difference apart relative to the permutation $P$ can be similarly pruned by
replacing $D^1$ with $D^j$, $j=1,2,\ldots,\ell$ in the above expressions.
Following this process, we can produce a well-pruned candidate set of possible
vector pairs that meet a given correlation threshold using $\ell$ matrix vector
products with matrices of order $n \times p$, where $p$ is chosen in practice
such that $p<<k$. These steps are formalized in Algorithm \ref{prune} below.

\begin{algorithm}\label{prune}
Input: data matrix $A\in \mathcal{R}^{m\times n}$, correlation threshold $t$,
truncated SVD rank $p<<k$.
\begin{enumerate}
\item Compute the truncated SVD $AV_{1:p} = U_{1:p}\mathrm{diag}(s_{1:p})$,
where $V_{1:p} = [v_1, v_2, \ldots, v_p]$
and $\mathrm{diag}(s_{1:p})$ denotes the diagonal matrix of the first $p$ singular values.
\item Compute permutation $P$ that orders points in $v_1$.
\item Compute $\ell$, the longest run of successive points in $P v_1$ within the interval $\sqrt{2(1-t)}/\sigma_1$.
\item Compute a set of candidate pairs $(\tilde{j},\tilde{j}+\tilde{i})$ with respect to the index order defined by the permutation $P$ that possibly meet the correlation threshold
\[
\bigcup_{\tilde{i}=1}^\ell
\left\{
(\tilde{j},\tilde{j}+\tilde{i}) : 
e_{\tilde{j}}^T(D^{\tilde{i}} P V_{1:p} )^2 s_{1:p}^2 \le 2(1-t)
\right\},
\]
where $e_{\tilde{j}}\in\mathcal{R}^n$ consists of $1$ in position $\tilde{j}$ and
zeros otherwise.
\item If there are judged to be  ``too many'' candidate pairs, increase $p$ and go to
step 1. Continue to increase $p$ in this way until the number of candidate
pairs is sufficiently small or stops decreasing (the best this algorithm can do).
\item Recover the non-permuted candidate column vector
indices $(j,i)$ from $j=e_{\tilde{j}}^T Pq$ and $i=e_{\tilde{j}+\tilde{i}}^TPq$ for every
pair $(\tilde{j},\tilde{j} + \tilde{i})$ in step 4, where
$q\in\mathcal{R}^n = [1,2,\ldots,n]^T$.
\item Compute the full correlation coefficient for each column vector pair identified in
the previous step, applying the correlation threshold to this reduced set of values.
\end{enumerate}
\end{algorithm}
Because the singular values $\sigma_j$, $j=1,2,\ldots,n$ are non-increasing and
decrease in proportion to variability in the data, it's often reasonable to
judge whether there are ``too many'' candidate pairs (step 5) after only a 1-d
projection $\tilde{i}=1$ in step 4 of the algorithm.

Algorithm \ref{prune} by construction guarantees that no pruned vector pair
meets the given correlation threshold; that is, the pruning can not produce any
``false negatives.''  The algorithm typically prunes the vast majority of pairs
of vectors below the correlation threshold with truncated singular value
decompositions of only relatively low rank $p$.  The choice of $p$ is a
balance between work in computing a truncated SVD and work required by the
$\ell$ matrix vector products of order $n\times p$ in step 4.



\section{General matrices and fast truncated SVD}\label{irlba}

We have so far assumed that the mean of each column vector of the $m\times n$
matrix $A$ is zero and the columns are scaled to have unit norm, but we really
want a method that works with general matrices $A$.  We also want a way to
efficiently compute a relatively low-rank truncated SVD of potentially large
matrices required by step 1 of Algorithm \ref{prune}.  Fortunately, both
desires are met by one algorithm.

The augmented implicitly restarted Lanczos bidiagonalization algorithm (IRLBA)
of Baglama and Reichel \cite{br} efficiently computes truncated singular value
decompositions of large dense or sparse matrices. Reference implementations are
available for R\cite{irlba-R}, Matlab\cite{irlba-M}, and Python\cite{irlba-P}.
The R implementation includes easy to use and efficient methods for restarting
to increase the subspace dimension (see step 5 of Algorithm \ref{prune}), and
for computing the SVD of scaled and centered matrices without explicitly
forming a new matrix.

We relax the column mean and scale assumptions without introducing much additional
computational or storage overhead as follows.  Let
$x=[x_1,x_2,\ldots,x_n]\in\mathcal{R}^n$ represent the vector of column means
of the matrix $A$, $w=[w_1^2,w_2^2,\ldots,w_n^2]\in\mathcal{R}^n$ be the vector
of squared column norms of $A$, and $W\in\mathcal{R}^{n\times n}$ a diagonal
matrix with diagonal entries $1/\sqrt{w_i^2 - m x_i^2}$.  Let
$e\in\mathcal{R}^m$ be a vector of all ones.  Then $(A - ex^T)W$ is a centered
matrix with zero column means, and scaled to have unit column norms, and
\[
\mathrm{cor}(A) = W^T (A-ex^T)^T (A-ex^T) W.
\]
IRLBA, based on the Lanczos process, is a method of iterated matrix-vector
products. The main idea behind efficient application of IRLBA to correlation
problems replaces matrix-vector products of the form $Az$ with $AWz - ex^TWz$ in
the iterations, implicitly working with a scaled and centered matrix without
forming it.  This comes at the cost of storing two additional length $n$ vectors and
computing two additional vector inner products per matrix-vector product.



\section{Numerical experiments}\label{examples}

%%<<eval=TRUE,echo=FALSE>>=
%%library(irlba)
%%
%%print("R code here")
%%@


\section{Conclusion}\label{conclusion}


%% XXX switch to bibtex / make citations uniform

\begin{thebibliography}{99}
\bibitem{br}
Baglama, J. and Reichel, L., Augmented Implicitly Restarted Lanczos Bidiagonalization Methods, SIAM J. Sci. Comput., 2005.
\bibitem{bhpt}
Bair, E., Hastie, T., Paul, D.., and Tibshirani, P., Prediction by supervised principal components, Journal of the American Statistical Association 101, no. 473, 2006.
\bibitem{latj}
Levin, Asriel U., Todd K. Leen, and John E. Moody. Fast pruning using principal components. (1993).
\bibitem{mnl}
Mueen, A., Nath, S., and Liu, J., Fast approximate correlation for
massive time-series data, in Proceedings of the 2010 ACM SIGMOD International
Conference on Management of data, 2010, pp. 171-182.
\bibitem{xstk1}
Xiong, Hui, et al. "Exploiting a support-based upper bound of Pearson's correlation coefficient for efficiently identifying strongly correlated pairs." Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2004.
\bibitem{xstk2}
Xiong, Hui, et al. "TAPER: A two-step approach for all-strong-pairs correlation query in large databases." Knowledge and Data Engineering, IEEE Transactions on 18.4 (2006): 493-508.
\bibitem{zf}
Zhang, J. and Feigenbaum, J, Finding Highly Correlated Pairs Efficiently with Powerful Pruning,
in Proceedings of ACM Conference on Information and Knowledge Management (CIKM), 2006, pp. 152-161.
\bibitem{irlba-R}
\url{https://cran.r-project.org/web/packages/irlba/index.html}
\bibitem{irlba-P}
\url{https://github.com/bwlewis/irlbpy}
\bibitem{irlba-M}
\url{http://www.math.uri.edu/~jbaglama/software/irlba.m}
\end{thebibliography}
\end{document}
