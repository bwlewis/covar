%
%% Instructions for processing this document into a PDF document
%%
%% FIRST NOTE, you need the 'biclust' package installed in R.
%% Some of the examples will come from data sets in this package.
%% You can install it from the R console with, for example:
%%
%% install.packages("biclust",repos="http://cran.case.edu")
%%
%% Now you can prepare the PDF of the paper with any of:
%%
%% From a shell:
%% R CMD Sweave --pdf draft.Rnw
%%
%% From an R prompt:
%% Sweave("draft.Rnw")
%% tools::texi2pdf("draft.tex")
%%
%% From RStudio:
%% click on the little "Compile PDF" button
%%
%% Note that you should probably run the above a few times because
%% LaTeX generally takes a few runs to get reference numbers right.
%%
\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcounter{algorithmctr}
\newenvironment{algorithm}{
   \refstepcounter{algorithmctr}
   \bigskip\noindent
   \textbf{Algorithm \thealgorithmctr\\}
}
{\bigskip}
\numberwithin{algorithmctr}{section}


\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}



\title{Efficient Thresholded Correlation using Truncated Singular Value Decomposition}
\author{
James Baglama\\
University of Rhode Island
\and
Michael Kane\\
Yale University
\and
Bryan Lewis\\
Paradigm4, Inc.\\
\and
Alex Poliakov\\
Paradigm4, Inc.
}

\date{}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

\begin{abstract}

Efficiently computing a subset of a correlation matrix consisting of values
above a specified threshold is important to many practical applications.
Real-world problems in genomics, machine learning, financial and other time
series and other applications can produce correlation matrices too large to
explicitly form and tractably compute. Often, only values corresponding to
highly-correlated vectors are of interest, and those values typically make up a
small fraction of the overall correlation matrix. We present a method based on
the singular value decomposition (SVD) and its relationship to the data
covariance structure that can efficiently compute thresholded subsets of very
large correlation matrices.

\end{abstract}


\section{Introduction}\label{intro}

Finding highly-correlated pairs among a large set of vectors is an important
part of many applications. For instance, subsets of highly-correlated vectors
of gene expression values may be used in the discovery of networks of genes
relevant to particular biological processes\cite{genomics}.  Identification of
highly-correlated pairs may be used in feature selection algorithms for machine
learning applications\cite{ml1, ml2} and are important to time series and
image processing applications\cite{timeseries, svd-similarity}.

The number of correlation coefficients grows quadratically with the number of
vectors. Simply computing all pairs of correlation coefficients and then
filtering out coefficients below a given threshold may not be computationally
feasible for high-dimensional data in modern genomics and other applications.
Considerable attention has been devoted to \emph{pruning
methods} that cheaply prune away all but the most highly-correlated pairs of
vectors.  For example the TAPER and related algorithms of Xiong et al. develop
pruning rules based on the frequency of mutually co-occurring observations (rows)
among vector pairs; see \cite{prune1, prune2} and the references therein.
Related methods approximate a set containing the most highly-correlated
pairs using hashing algorithms\cite{prune3}. A number of other methods arrive
at approximate sets of the most highly-correlated pairs using
dimensionality-reduction techniques like the discrete Fourier
transform\cite{timeseries}. The method of Wu, et al.\cite{svd-similarity},
conceptually very similar to our approach, uses the singular value
decomposition (SVD) to find all pairs of close vectors with respect to a
distance metric (instead of correlation).  Our method finds all of the most
highly-correlated vector pairs with respect to a given threshold by pruning
along an intuitive path of decreasing variance defined by the data and computed
by a truncated SVD.

Consider a real-valued data matrix $A$ consisting of $m$ observations of $n$
column vectors.  Denote the columns of $A$ as $a_j\in\mathcal{R}^m$ for
$j=1,2,\ldots,n$, $A=[a_1, a_2, \ldots, a_n]\in\mathcal{R}^{m\times n}$, and
let $k=\mathrm{rank}(A)$.  Assume that the mean of each column $a_j$ is zero
and that each column has unit norm $\|a_j\| = 1$. Here and below, $\|\cdot\|$
denotes the Euclidean vector norm. Then the Pearson sample correlation matrix
$\mathrm{cor}(A)=A^TA$.  Note that under these assumptions the sample
correlation and covariance matrices are the same up to a constant multiple.
Section \ref{irlba} illustrates how to relax the unit norm and zero mean
assumptions in practice.

Let $A=U\Sigma V^T$ be a singular value decomposition of $A$, where
$U\in\mathcal{R}^{m\times k}$, $V\in\mathcal{R}^{n\times k}$,
$U^TU = V^TV = I$,
and $\Sigma\in\mathcal{R}^{k\times k}$ is a diagonal matrix with
diagonal entries $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_k > 0$.  For
later convenience, we write $s_{1:p}=[\sigma_1, \sigma_2, \ldots, \sigma_p]^T$
to be the vector of the first $p\le k$ singular values along the diagonal of
$\Sigma$.  Denote the columns of $V$ as $V=[v_1, v_2, \ldots, v_k]$, where each
$v_j\in\mathcal{R}^n$, $j=1,2,\ldots,k$.  Then $\mathrm{cor}(A) = A^TA =
V\Sigma^2 V^T$ is a symmetric eigenvalue decomposition of the correlation
matrix with nonzero eigenvalues $\sigma_1^2, \sigma_2^2, \ldots, \sigma_k^2$.
The span of the columns of $V$ form an orthonormal basis of the range of the
correlation matrix $A^TA$.

Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix $A$
(including possibly the case $i=j$). Denote the correlation of these two
vectors by $\mathrm{cor}(a_i,a_j) = a_j^T a_i$.  The following simple lemma
establishes a relationship between correlation and Euclidean distance between
two zero mean, unit norm vectors.

\begin{lemma}
\label{distlemma}
Let $a_i$ and $a_j$, $1\le i,j\le n$ be any two columns of the matrix
$A$. Then
\begin{equation}\label{cordist}
\mathrm{cor}(a_i,a_j) = a_i^Ta_j = 1 - \|a_i - a_j\|^2/2.
\end{equation}
In particular, for a given correlation threshold $t$, $0<t<1$, if
$\|a_i - a_j\|^2 > 2(1-t)$ then
$\mathrm{cor}(a_i, a_j) < t$.
\end{lemma}
%%\begin{proof}
%%\begin{eqnarray*}
%%\|a_i - a_j\|^2 &=& (a_i - a_j)^T(a_i - a_j) \\
%%&=& a_i^T a_i + a_j^T a_j - 2a_i^T a_j \\
%%&=& \|a_i\|^2 + \|a_j\|^2 - 2a_i^T a_j \\
%%&=& 2 - 2a_i^T a_j.
%%\end{eqnarray*}
%%The lemma follows by solving for $a_i^Ta_j$.
%%\end{proof}

Lemma \ref{distlemma} equates the problem of finding pairs of highly correlated
column vectors from the matrix $A$ with a problem of finding pairs of vectors
sufficiently close together. For example, correlation values larger than 0.99
may only be associated with column vectors from the matrix $A$ whose Euclidean
distance is less than $\sqrt{0.02}$.

The following lemma expresses the Euclidean distance between any two columns of
the matrix $A$ as a weighted sum of projected interval distances along the SVD
basis vectors forming the columns of $V$.
\begin{lemma}
Let $a_i$ and $a_j$
be any two columns of the matrix
$A$, $1\le i,j\le n$. Then
\begin{equation}\label{vdist}
\|a_i - a_j\|^2 =
\sigma_1^2 (e_i^Tv_{1} - e_j^Tv_{1})^2 + 
\sigma_2^2 (e_i^Tv_{2} - e_j^Tv_{2})^2 + \cdots + 
\sigma_k^2 (e_i^Tv_{k} - e_j^Tv_{k})^2,
\end{equation}
\end{lemma}
where here and below, $e_{j}\in\mathcal{R}^n$ represents the $j$th unit basis
vector consisting of $1$ in position $j$ and zeros otherwise.
(Thus, $e_j^Tv_{k}$ is the $j$th position in the vector $v_k$--that is, the
$[j,k]$ entry in the matrix $V$.)

%%\begin{proof}
%%\begin{align*}
%%\|a_i - a_j\|^2 &= (a_i - a_j)^T(a_i - a_j) \\
%%&= a_i^Ta_i + a_j^T a_j - 2a_i^Ta_j \\
%%&= \sigma_1^2 v_{1i}^2 + \sigma_2^2 v_{2i}^2 + \cdots + \sigma_k^2 v_{ki}^2 \\
%%&\phantom{=} + \sigma_1^2 v_{1j}^2 + \sigma_2^2 v_{2j}^2 + \cdots + \sigma_k^2 v_{kj}^2 \\
%%&\phantom{=} -2(\sigma_1^2 v_{1i}v_{1j} + \sigma_2^2 v_{2i}v_{2j} + \cdots + \sigma_k^2 v_{ki}v_{kj}) \\
%%&= \sigma_1^2 (v_{1i}^2 + v_{1j}^2 - 2v_{1i}v_{1j}) \\
%%&\phantom{=} + \sigma_2^2 (v_{2i}^2 + v_{2j}^2 - 2v_{2i}v_{2j}) \\
%%&\phantom{=} + \cdots \\
%%&\phantom{=} + \sigma_k^2 (v_{ki}^2 + v_{kj}^2 - 2v_{ki}v_{kj}) \\
%%&= \sigma_1^2 |v_{1i} - v_{1j}|^2
%% + \sigma_2^2 |v_{2i} - v_{2j}|^2
%% + \cdots
%% + \sigma_k^2 |v_{ki} - v_{kj}|^2.
%%\end{align*}
%%\end{proof}

Note that each term in the sum is nonnegative and the weights $\sigma_j^2$,
$j=1,2,\ldots,k$ are nonincreasing and defined by the covariance structure of
the data.  Given a correlation threshold $t$, $0<t<1$, Equations \ref{cordist}
and \ref{vdist} together suggest that pairs of vectors can be excluded from
consideration whenever their projected distance is too large.  For example if
$\sigma_1^2 (e_i^Tv_{1} - e_j^Tv_{1})^2 > 2(1-t)$, then we can conclude from
just a few scalar values that the column vectors $a_i$ and $a_j$ do not meet the
correlation threshold $t$.
In practice, many pairs of vectors may be pruned in this fashion by
inspecting distances in low-dimensional subspaces, substantially reducing the
complexity of computing thresholded correlation matrices.

\section{Efficient pruning using truncated SVD}

Let $t$ be a given correlation threshold, $0<t<1$, and let
$P\in\mathcal{R}^{n\times n}$ be a permutation matrix that orders the entries
of $v_1$ in increasing order.  For example, Figure \ref{fig1} displays the
ordered entries of $Pv_1$ for the small ``BicatYeast'' example (where
$A\in\mathcal{R}^{70\times 419}$) from Section \ref{examples}.  The lines in
the plot illustrate the interval associated with the correlation threshold
$t=0.95$ placed at an arbitrary vertical axis location.

\begin{figure}[!ht]
\begin{center}
<<eval=TRUE, echo=FALSE, fig=TRUE, height=5, width=7>>=
library(biclust)
data(BicatYeast)
A=t(BicatYeast)
s=svd(A)
i=order(s$v[,1])
plot(s$v[i,1], ylab=expression(P * v[1]), xlab="Sorted index", pch='.')
t=0.95
abline(h=c(-0.05,-0.05+sqrt(2*(1-t))/s$d[1]),col=2,lwd=0.5)
@
\caption{
Example plot of the ordered entries of $v_1$ for the BicatYeast example from
Section \ref{examples}. The red lines illustrate the interval corresponding
threshold $t=0.95$ placed at an arbitrary location on the vertical axis. Points
farther apart than this interval correspond to column vectors that do not meet
the correlation threshold.
\label{fig1}
}
\end{center}
\end{figure}

Let $D^i\in\mathcal{R}^{n-i, n}$, $i=1,2,\ldots,n-1$, be the $i$th order finite
difference matrix with $-1$ along the diagonal, $1$ along the $i$th
super-diagonal, and zeros elsewhere.  Consider just adjacent points of $P v_1$.
Then $D^1P v_1$ consists of the differences of adjacent entries in $P v_1$.
Entries such that $\sigma_1^2 (D^1 P v_1)^2 > 2(1-t)$ correspond to pairs of
vectors that do not meet the correlation threshold, where here and below
an exponent applied to a vector denotes element-wise exponentiation.

Even a 1-d projected interval distance threshold may prune many possible
non-adjacent vector pairs with respect to the ordering defined by $P$ as
illustrated by the example in Figure \ref{fig1}.  However, this observation is
unlikely to rule out many adjacent pairs of vectors in most problems.  For
instance, the maximum distance between adjacent points shown in the example in
Figure \ref{fig1} is less than $\sqrt{2(1-t)}/\sigma_1$, and no pruning of
adjacent pairs of vectors with respect to the ordering $P$ occurs.

Including more terms from Equation \ref{vdist} increases our ability to prune
adjacent pairs of vectors below the correlation threshold with respect to the
ordering defined by $P$.  Including only one more term finds that 49\% of
adjacent vectors fall below the correlation threshold of $0.95$ in the
example shown in Figure \ref{fig1} and are pruned by identifying indices such
that
\[
\sigma_1^2 (D^1 P v_1)^2 +
\sigma_2^2 (D^1 P v_2)^2 > 2(1-t).
\]
Note that we use the permutation $P$ defined by the order of vector $v_1$
throughout. Including ten terms prunes over 96\% of adjacent vector pairs,
leaving only 15 candidate pairs of adjacent vectors that might possibly
meet the correlation threshold.
In general, using $p\le k$ terms to prune pairs of vectors below the
correlation threshold boils down to evaluating the expression
\begin{equation}\label{proj}
(D^1 P V_{1:p} )^2 s_{1:p}^2 > 2(1-t),
\end{equation}
where 
$V_{1:p}$ denotes the first $p$ columns of the
matrix $V$, $s_{1:p}$ the vector of the first $p$ singular values.

Let $\ell$ be the longest run of successive points in $P v_1$ within the
interval $\sqrt{2(1-t)}/\sigma_1$.  The quantity $\ell$ may, for example, be
obtained by rolling the interval over all the points in Figure \ref{fig1} and
counting the maximum number of points that lie within the interval.  With
respect to the ordering $P$, $\ell$ represents the biggest index difference
that pairs of indices corresponding to correlated vectors above the threshold
can exhibit.

Pairs of vectors below the correlation threshold that lie more than one index
difference apart relative to the permutation $P$ can be similarly pruned by
replacing $D^1$ with $D^j$, $j=2,3,\ldots,\ell$ in the above expressions.
Following this process, we can produce a well-pruned candidate set of vector
pairs that contains all pairs that meet a given correlation threshold using
$\ell$ matrix vector products with matrices of order $n \times p$, where $p$ is
chosen in practice such that $p<<k$. Setting $p=10$ for the example shown in
Figure \ref{fig1} prunes all but $431$ possible vector pairs out of a total
$419(419-1)/2 = 87,571$ with $\ell=138$.

These steps are formalized in Algorithm \ref{prune} below. The algorithm
proceeds in two main parts: steps 1--6 prune pairs of vectors down to a small
candidate set that may meet the correlation threshold; step 7
computes the correlation values and applies the threshold across the reduced
set of vector pairs.

\begin{algorithm}\label{prune}
Input: data matrix $A\in \mathcal{R}^{m\times n}$ with columns scaled to have
zero mean and unit norm, correlation threshold $t$,
truncated SVD rank $p<<k$.
\begin{enumerate}
\item Compute the truncated SVD
$AV_{1:p} = U_{1:p}\mathrm{diag}(s_{1:p})$,
where\\
$V_{1:p}=[v_1, v_2, \ldots, v_p]$
and $\mathrm{diag}(s_{1:p})$ denotes the diagonal matrix of the first $p$ singular values.
\item Compute permutation $P$ that orders points in $v_1$.
\item Compute $\ell$, the longest run of successive points in $P v_1$ within the interval $\sqrt{2(1-t)}/\sigma_1$.
\item Compute a set of candidate index pairs $(\tilde{j},\tilde{j}+\tilde{i})$ with respect to the index order defined by the permutation $P$ that possibly meet the correlation threshold
\[
\bigcup_{\tilde{i}=1}^\ell
\left\{
(\tilde{j},\tilde{j}+\tilde{i}) : 
e_{\tilde{j}}^T(D^{\tilde{i}} P V_{1:p} )^2 s_{1:p}^2 \le 2(1-t)
\right\},
\]
where $\tilde{j}=1,2,\ldots,n-\tilde{i}$.
\item If there are judged to be  ``too many'' candidate pairs, increase $p$,
compute $AV_{1:p} = U_{1:p}\mathrm{diag}(s_{1:p})$ and go to step 4.
Continue to increase $p$ in this way until the number of candidate
pairs is sufficiently small or stops decreasing (the best this algorithm can do).
\item Recover the non-permuted candidate column vector
indices $(j,i)$ from $j=e_{\tilde{j}}^T Pq$ and $i=e_{\tilde{j}+\tilde{i}}^TPq$ for every
pair $(\tilde{j},\tilde{j} + \tilde{i})$ in step 4, where
$q\in\mathcal{R}^n = [1,2,\ldots,n]^T$.
\item Compute the full correlation coefficient for each column vector pair identified in
the previous step, applying the correlation threshold to this reduced set of values.
\end{enumerate}
\end{algorithm}
Because the singular values $\sigma_j$, $j=1,2,\ldots,n$ are non-increasing and
decrease in proportion to variability in the data, it's often reasonable to
judge whether there are ``too many'' candidate pairs (step 5) after only a 1-d
projection $\tilde{i}=1$ in step 4 of the algorithm.

Algorithm \ref{prune} guarantees that no pruned vector pair exceeds the given
correlation threshold--the pruning does not produce false negatives.  The
algorithm typically prunes the vast majority of pairs of vectors below the
correlation threshold with truncated singular value decompositions of only
relatively low rank $p$.  The choice of $p$ represents a balance between work
in computing a truncated SVD and evaluating $\ell$ rank $p$ matrix products in
Step 4 (each increasing in work as $p$ increases), and the work required to
filter the thresholded values in step 7 (decreasing work as $p$ increases).

Most of the floating point operations occur in step 4 of Algorithm \ref{prune}.
Fortunately the $\ell$ evaluations in step 4 are independent of each other and
can easily be computed in parallel. See the reference R
implementation\cite{sup} for an example.


\section{General matrices and fast truncated SVD}\label{irlba}

We have so far assumed that the mean of each column vector of the $m\times n$
matrix $A$ is zero and the columns are scaled to have unit norm, but we really
want a method that works with general matrices $A$.  We also want a way to
efficiently compute a relatively low-rank truncated SVD of potentially large
matrices required by step 1 of Algorithm \ref{prune}.  And the truncated SVD
method should be able to cheaply restart to compute additional singular vectors
as required in step 5 of Algorithm \ref{prune}. Fortunately all desires are met
by one algorithm.

The augmented implicitly restarted Lanczos bidiagonalization algorithm\break(IRLBA)
of Baglama and Reichel \cite{irlba} efficiently computes truncated singular value
decompositions of large dense or sparse matrices. Reference implementations are
available for R\cite{irlbar}, Matlab\cite{irlbam}, and Python\cite{irlbap}.

We can relax the column mean and scale assumptions without introducing much
additional computational or storage overhead as follows. Assume $A$ is an
$m\times n$ real-valued matrix without constant-valued columns. Let
$z=[z_1,z_2,\ldots,z_n]\in\mathcal{R}^n$ represent the vector of column means
of the $m\times n$ matrix $A$, $w=[w_1^2,w_2^2,\ldots,w_n^2]\in\mathcal{R}^n$
be the vector of squared column norms of $A$, and $W\in\mathcal{R}^{n\times n}$
a diagonal matrix with diagonal entries $1/\sqrt{w_i^2 - m z_i^2}$.  Let
$e\in\mathcal{R}^m$ be a vector of all ones.  Then $(A - ez^T)W$ is a centered
matrix with zero column means and scaled to have unit column norms, and
\[
\mathrm{cor}(A) = W^T (A-ez^T)^T (A-ez^T) W.
\]
The IRLBA, based on the Lanczos process, is a method of iterated matrix-vector
products. The main idea behind efficient application of IRLB to correlation
problems replaces matrix-vector products of the form $Ax$ with $AWx - ez^TWx$ in
the iterations, implicitly working with a scaled and centered matrix without
forming it.  This comes at the cost of storing two additional length $n$ vectors and
at most computing two additional vector inner products per matrix-vector product.
The R implementation\cite{irlbar} of IRLBA includes arguments for centering and
scaling the input matrix.

A full/naive thresholded correlation computation requires $O(n^2 m)$ flops.
Let $p$ be the selected IRLBA dimension. Then the proposed method requires
$O(\ell n p)$ flops, where $\ell$ is the longest run of ordered entries in
$v_1$ that meet the 1-d projected distance threshold $\sqrt{2(1-t)}/\sigma_1$,
plus the flops required by IRLBA and the post-processing step 7 (also $O(np)$).



\section{Numerical experiments}\label{examples}

We evaluated the algorithm using three public gene expression and methylation
datasets.  Most tests were performed on a desktop PC with a single AMD
A10-7850K 3.7$\,$GHz Athlon quad-core CPU and 16$\,$GB 1333$\,$MHz unbuffered
RAM running the Ubuntu 12.04.5 GNU/Linux OS. Tests were performed with 64-bit R
version 3.1.2 using double-precision arithmetic and the OpenBLAS library
version 0.1alpha2.2-3 (based on Goto's BLAS version 1.13) with {\tt
OMP\_NUM\_THREADS=4}. The last experiment includes a test run on a Linux
cluster as described in the text.

All examples use the reference R implementation, {\tt tcor()}, which can be
installed for the R language from the development GitHub repository using the
{\tt devtools} package with:

<<eval=FALSE,echo=TRUE>>=
devtools::install_github("bwlewis/tcor")
@

The native R {\tt cor()} function is a general function that can compute
several different types of correlation matrices including Pearson, Spearman,
and Kendall and includes a number of options for dealing with missing values.
Because of its flexibility, the native {\tt cor()} function does not always
compute Pearson correlation matrices in the most computationally-efficient way.

Correlation matrix computation can benefit substantially from use of optimized
BLAS Level 3 operations (matrix multiplication), achieving high utilization of
available CPU floating point capacity. The {\tt tcor()} function also benefits
from optimized BLAS routines, but not as much, as it mostly makes use of Level
2 operations and Level 3 operations on sequences of smaller problems.

We compare the {\tt tcor()} function with brute-force Pearson correlation
matrix computation computed in the fastest way we can, directly using matrix
multiplication instead of using the slower {\tt cor()} function.


\subsection{Small BicatYeast example}

The first example is a tiny proof of concept that provides the data for Figure
\ref{fig1} and establishes our timing and memory use measurement methodology.
Wall-clock times are reported in seconds and peak memory use in excess of
memory required to store the input data matrix is reported in megabytes.

Anyone can quickly run the example to experiment with the algorithm and
compare its output with brute force results. The example uses the {\tt
BicatYeast} data from the {\tt biclust} R package and related {\tt BicAT}
software\cite{biclust, bicat}. The data consist of 70 experiments involving the
Saccharomyces Cerevisiae yeast measuring gene expression data across 419
Affymetrix probes, represented as a $70\times 419$ real-valued matrix.  The
example finds all pairs of columns (Affymetrix probes) that meet or exceed a
Pearson correlation value of 0.95.  We use a projected dimension $p=10$ in {\tt tcor()}.
\begin{small}
<<eval=TRUE,echo=TRUE>>=
library(irlba)
source("tcor.R")  # XXX Replace later with tcor pacakge
library(biclust)  # for the BicatYeast data
data(BicatYeast)  # in gene by sample orientation
A = t(BicatYeast) # to correlate genes to genes
threshold = 0.95

# Compute using tcor():
m1  = sum(gc()[,2])  # Memory in megabytes currently in use
t1  = proc.time()
tx  = tcor(A, t=threshold, p=10)
t1  = (proc.time() - t1)[3] # Time in seconds
m1  = sum(gc()[,6]) - m1    # Peak excess memory use during the test

# Fast brute force full correlation:
m2  = sum(gc()[,2])
t2  = proc.time()
mu = colMeans(A)
s  = sqrt(apply(A, 2, crossprod) - nrow(A) * mu ^ 2)
cs = scale(A, center=mu, scale=s)  # scale and center the data
cx = crossprod(cs)                 # full correlation matrix
cx  = cx * upper.tri(cx)    # ignore symmetry and diagonal
pairs = which(cx >= threshold, arr.ind=TRUE)
t2  = (proc.time() - t2)[3]
m2  = sum(gc()[,6]) - m2
@
\end{small}
Brute-force computation outperforms {\tt tcor()} in this tiny first example,
both in speed and slightly in memory use.  Each algorithm identifies the same
115 unique vector pairs that exceed the specified correlation threshold of
0.95, with comparative timing and memory use shown in the following table.

<<echo=FALSE, results=tex>>=
require(xtable)
ans = data.frame(Method=c("tcor","brute force"), `Wall-clock time (s)`=c(t1, t2), `Peak excess memory use (MB)`=c(m1, m2), check.names=FALSE)
print(
    xtable(ans,
        caption = "BicatYeast results, threshold=0.95", label = "BicatYeast", caption.placement = "top")
  ,size="small", table.placement="ht", include.rownames=FALSE)
@


\subsection{TCGA Gene Expression Examples}

Performance benefits from the {\tt tcor()} function become apparent with more
data.  The next example uses RPKM-normalized gene expression data measured
across 20,532 genes from 878 breast cancer tumor samples obtained from the
Broad GDAC Cancer Genome Atlas dashboard\cite{gdac}. See the supplemental
code\cite{sup} (also embedded in this Sweave document) for examples of
efficiently converting the downloaded sample expression data into an R matrix.

Eight columns with constant zero values were removed from the data,
leaving an $878\times 20524$ matrix. The example computes
pairs of gene expression column vectors with correlation values of 0.99
or more. We use the same {\tt tcor} projection dimension of 10 and performance
measurement methodology as used in the first example.

Both methods find the same 3,572 pairs of vectors. The fast brute-force
computation is slightly faster than the {\tt tcor()} function largely due to
the still relatively small input matrix and better utilization of optimized
BLAS routines. But the {\tt tcor()} function uses far less memory.  (For
comparison, R's {\tt cor()} function took about 372 seconds and about
3200$\,$MB RAM on the same problem on the same test system.)

<<echo=FALSE, results=tex>>=
# --------------------------------------------------------------------------------------------------
# The first time you run this, you need to download the data as outlined in the
# following commented-out section. This code creates a file named
# 'rpkm.RData' used subsequently.
#
# To run any of the remaining (large) examples, first set the system environment
# variable RUN_EXAMPLES=TRUE (otherwise they won't run and representative results
# from Bryan's test machine will be printed).
#
#library(doMC)
# Download and extract this file with, for example:
# wget  http://gdac.broadinstitute.org/runs/stddata__2015_08_21/data/BRCA/20150821/gdac.broadinstitute.org_BRCA.Merge_rnaseq__illuminahiseq_rnaseq__unc_edu__Level_3__gene_expression__data.Level_3.2015082100.0.0.tar.gz
# tar zxf gdac.broadinstitute.org_BRCA.Merge_rnaseq__illuminahiseq_rnaseq__unc_edu__Level_3__gene_expression__data.Level_3.2015082100.0.0.tar.gz
# cd gdac.broadinstitute.org_BRCA.Merge_rnaseq__illuminahiseq_rnaseq__unc_edu__Level_3__gene_expression__data.Level_3.2015082100.0.0
# R --slave < read.R   # (run this file)
#f = "BRCA.rnaseq__illuminahiseq_rnaseq__unc_edu__Level_3__gene_expression__data.data.txt"
#N = 20534  # total number of lines
#h = 2      # header lines
# Read the first header line with sample IDs:
#id = as.vector(read.table(f, sep="\t", stringsAsFactors=FALSE, header=FALSE, nrows=1))
# Read the 2nd header line with sample type (raw, RPKM, etc.)
#header = as.vector(read.table(f, sep="\t", stringsAsFactors=FALSE, skip=1, header=FALSE, nrows=1))
# Read the rest of the file in parallel. I prefer doMC/foreach over mclapply
# for this because foreach can combine results incrementally.
#cores = 4
#registerDoMC(cores)
#block = floor((N - h)/cores)
#t1 = proc.time()
#ans = foreach(j=1:cores, .combine=rbind) %dopar%
#{
#    skip = block * (j - 1) + h
#    nrows = ifelse(j == cores, -1, block)
#    read.table(f, sep="\t", stringsAsFactors=FALSE, skip=skip, header=FALSE, nrows=nrows)
#}
#names(ans) = header
#print(proc.time() - t1)
# (All timings shown for a quad-core Athlon home PC reading from an older
# OCZ REVODRIVE3 X2 SSD drive (ca 2014).)
#   user  system elapsed
#494.549   4.134 131.232
# Compare with sequential approach:
#t1 = proc.time()
#x = read.table(f, sep="\t", stringsAsFactors=FALSE, skip=1, header=TRUE)
#print(proc.time() - t1)
#   user  system elapsed
#517.430   1.318 519.553
# Create a samples x gene matrix of RPKM values
#genes = ans[, 1] # gene labels
#idx   = grep("RPKM", names(ans))    # Select columns with RPKM values
#rpkm     = t(as.matrix(ans[, idx]))
# Label the coordinates by sample ID and gene:
#rownames(rpkm) = id[idx]
#colnames(rpkm) = genes
#save(rpkm, file="rpkm.RData")
# --------------------------------------------------------------------------------------------------
# Default values:
t1=40.34
t2=35.60
m1=39.10
m2=9595.10

if(Sys.getenv("RUN_EXAMPLES")=="TRUE")
{
load("rpkm.RData")  # rpkm expression matrix

# The data have a few columns of all zeros that need to be removed
n = ncol(rpkm)
rpkm = rpkm[, colMeans(rpkm) > 0]

threshold = 0.99     # correlation threshold
xx  = gc()           # clear
m1  = sum(gc()[,2])  # Memory in megabytes currently in use
t1  = proc.time()
tx  = tcor(rpkm, threshold, 11)
t1  = (proc.time() - t1)[3] # Time in seconds
m1  = sum(gc()[,6] - m1)    # Peak excess memory use during the test

# Compare with the fastest way we know of to compute a thresholded correlation
# matrix by brute force. This approach makes heavy use of fast multi-threaded
# BLAS routines. This approach is much faster than R's default `cor` function.
xx  = gc()
m2  = sum(gc()[,2])  # Memory in megabytes currently in use
t2 = proc.time()
mu = colMeans(rpkm)
s  = sqrt(apply(rpkm, 2, crossprod) - nrow(rpkm) * mu ^ 2) # col norms of centered matrix
cs = scale(rpkm, center=mu, scale=s)  # scale and center the data
bx = crossprod(cs)                    # full correlation matrix
diag(bx) = 0                          # ignore diagonal (all ones)
pairs = which(bx >= threshold, arr.ind=TRUE)
t2  = (proc.time() - t2)[3] # Time in seconds
m2  = sum(gc()[,6] - m2)    # Peak excess memory use during the test
} # RUN_EXAMPLES=TRUE

require(xtable)
ans = data.frame(Method=c("tcor","brute force"), `Wall-clock time (s)`=c(t1, t2), `Peak excess memory use (MB)`=c(m1, m2), check.names=FALSE)
print(
    xtable(ans,
        caption = "TCGA BRCA gene expression results (878 samples x 20,542 genes, threshold=0.99)", label = "TCGA1", caption.placement = "top")
  ,size="small", table.placement="ht", include.rownames=FALSE)
@


We simulated a data set with a larger number of samples by simply replicating
the rows of the gene expression matrix 27 times to obtain a matrix with 23,706
rows and 20,524 columns, noting that the simulated data have the same rank (of
at most 878) and the same expected number of vector pairs meeting the
correlation threshold as the original data. Each method again computed the same
set of  3,572 pairs of vectors meeting the correlation threshold of 0.99, but
the {\tt tcor()} function outperforms the brute force method in both compute
time and memory consumption.


<<echo=FALSE, results=tex>>=
# Default values:
t1=372.69
t2=546.06
m1=768.3
m2=9600.2

if(Sys.getenv("RUN_EXAMPLES")=="TRUE")
{
  # simulated data, replicate rows 27x
}
require(xtable)
ans = data.frame(Method=c("tcor","brute force"), `Wall-clock time (s)`=c(t1, t2), `Peak excess memory use (MB)`=c(m1, m2), check.names=FALSE)
print(
    xtable(ans,
        caption = "TCGA simulated gene expression results (23,706 rows x 20,542 columns, threshold=0.99)", label = "TCGA2", caption.placement = "top")
  ,size="small", table.placement="ht", include.rownames=FALSE)
@


\subsection{TCGA DNA methylation example}

The following example uses calculated beta values from the Illumina Human
Methylation450 BeadChip of 80 Adenoid Cystic Carcinoma (ACC) tumor samples
obtained from the Broad GDAC Cancer Genome Atlas dashboard\cite{gdac}. See the
supplemental code\cite{sup} for examples of efficiently converting the
downloaded sample methylation data into an R matrix.

We removed 91,563 columns with all constant or missing values from the data,
leaving an $80\times 394014$ matrix. The example computes
pairs of methylation beta value column vectors with correlation values of 0.99
or more. We use the same {\tt tcor} projection dimension of 10 and performance
measurement methodology as used in the first example.

Algorithm \ref{prune} identified 913,601 pairs of vectors with correlation
values of 0.99 or more in about three hours on our test PC. A brute force
algorithm would need to evaluate over 155 billion vector pairs; the projection
of Algorithm \ref{prune} limited the search space to only about 4 million
candidate pairs, from which the 913,601 correlated pairs were found.

We illustrate that steps 4--7 of Algorithm \ref{prune} can be computed in
parallel by comparing the above results run on our reference test PC with
results run on a modest GNU/Linux cluster at Paradigm4, Inc. The cluster
consisted of four computers connected by 10 gigabit ethernet. Each computer
contained two Intel Xeon E5-2650 2$\,$GHz CPUs (16 physical CPU cores per
computer) and 64$\,$GB ECC registered RAM running at 1,600$\,$MHz running
CentOS 6.6 and 64-bit R version 3.2.2 with the reference R BLAS library.

No code change was required to run in parallel on the cluster. The reference R
{\tt tcor()} implementation uses R's {\tt foreach}\cite{foreach} framework to
run on arbitrary parallel configurations (``back ends''). We simply registered
a {\tt doRedis}\cite{doredis} parallel back end with 16 R workers per computer
(64 total R worker processes) prior to invoking {\tt tcor()}.

The results computed on the Linux cluster are marked ``tcor (64 core cluster)''
below. The cluster example computed the filtering step 7 of Algorithm
\ref{prune} in parallel across the 64 remote R processes. That approach incurs,
in this example, an additional memory overhead of about 16$\,$GB because each
remote R process requires access to a copy of the data matrix $A$. More thrifty
adaptations of the algorithm are possible and could, for example, share the
memory required by $A$ between worker R processes on each computer.

<<echo=FALSE, results=tex>>=
# Default values:
t1=10449.670
t3=482
t4=NA
m1=1261.6
m3=994.3
m4=42282.4

if(Sys.getenv("RUN_EXAMPLES")=="TRUE")
{
 # methylation data
}
require(xtable)
ans = data.frame(Method=c("tcor (4 core PC)", "tcor (64 core cluster)", ""), `Wall-clock time (s)`=c(t1, t3, t4), `Memory use (MB)`=c(m1, m3, m4), " "=c("","(local)","(remote)"), check.names=FALSE)
print(
    xtable(ans,
        caption = "TCGA ACC Methylation450 (80 rows x 394,014 columns, threshold=0.99). Linux cluster results include master R process memory use (local) plus total peak memory used by all 64 worker R processes (remote).", label = "TCGA3", caption.placement = "top")
  ,size="small", table.placement="ht", include.rownames=FALSE, NA.string="")
@
From a set of almost 400 thousand vectors, Algorithm \ref{prune} finds 913,601
pairs of vectors with correlation values of 0.99 or more on a modest Linux
cluster in about 8 minutes. The same algorithm also runs well in constrained
memory settings.

%%<<eval=TRUE,echo=FALSE>>=
%%library(irlba)
%%
%%print("R code here")
%%@




\section{Extensions}\label{extensions}

XXX

1. The main ideas in Algorithm \ref{prune}, although applied to correlation,
easily extend to any distance metric.


2. Anti-correlated pairs? Anyone?



\section{Conclusion}\label{conclusion}








\begin{thebibliography}{99}

%% Related use of SVD in other methods
%\bibitem{spca}
%Bair, Eric, et al. Prediction by supervised principal components. Journal of the American Statistical Association 101.473 (2006).

%% irlba
\bibitem{irlba}
Baglama, James, and Lothar Reichel. Augmented implicitly restarted Lanczos bidiagonalization methods. SIAM Journal on Scientific Computing 27.1 (2005): 19-42.

\bibitem{irlbam}
Baglama, James,  {\url{http://www.math.uri.edu/~jbaglama/software/irlba.m}}.

%% BicatYeast data
\bibitem{bicat} Barkow, S., Bleuler, S., Prelic, A., Zimmermann, P., and E. Zitzler. BicAT: a biclustering analysis toolbox, Bioinformatics, 2006 22(10):1282-1283.

%% BROAD GDAC dashboard
\bibitem{gdac}{Broad Institute TCGA Genome Data Analysis Center (2015): Analysis-ready standardized TCGA data from Broad GDAC Firehose stddata\_\_2015\_06\_01 run. Broad Institute of MIT and Harvard. Dataset. \url{http://dx.doi.org/10.7908/C1251HBG}}

%% Applications in machine learning
\bibitem{ml1}
Hall, Mark A. Correlation-based feature selection for machine learning. Diss. The University of Waikato, 1999.

%% R's biclust pacakge
\bibitem{biclust}
Kaiser, Sebastian, Santamaria, Rodrigo, Khamiakova, Tatsiana, Sill, Martin,
Theron, Roberto, Quintales, Luis, Leisch, Friederich, and De Troyer, Ewoud, 
\url{https://cran.r-project.org/web/packages/biclust}.

%% IRLBA implementations
\bibitem{irlbap}
Kane, Michael and Lewis, Bryan,  \url{https://github.com/bwlewis/irlbpy}.

%% doredis
\bibitem{doredis}
Lewis, Bryan. The doRedis package for R. \url{https://cran.r-project.org/web/packages/doRedis}.

%% IRLBA implementations and supplemental code
\bibitem{irlbar}
Lewis, Bryan. The irlba package for R.  \url{https://cran.r-project.org/web/packages/irlba}.

\bibitem{sup}
Lewis, Bryan. The tcor package for R.  \url{https://github.com/bwlewis/tcor}.

%% Approximate correlation
\bibitem{timeseries}
Mueen, Abdullah, Suman Nath, and Jie Liu. Fast approximate correlation for massive time-series data. Proceedings of the 2010 ACM SIGMOD International Conference on Management of data. ACM, 2010.

%% Applications in genomics
\bibitem{genomics}
Song, Lin, Peter Langfelder, and Steve Horvath. Comparison of co-expression measures: mutual information, correlation, and model based indices. BMC bioinformatics 13.1 (2012): 328.

%% foreach
\bibitem{foreach}
Weston, Steve. The foreach package for R. \url{https://cran.r-project.org/web/packages/foreach/vignettes/foreach.pdf}.

%% SVD similarity -- extremely similar to the approach in this paper!
%% Applied to images
\bibitem{svd-similarity}
Wu, Daniel, et al. Efficient retrieval for browsing large image databases. Proceedings of the fifth international conference on Information and knowledge management. ACM, 1996.

%% Other correlation pruning methods
\bibitem{prune1}
Xiong, Hui, et al. TAPER: A two-step approach for all-strong-pairs correlation query in large databases. Knowledge and Data Engineering, IEEE Transactions on 18.4 (2006): 493-508.
\bibitem{prune2}
Xiong, Hui, Mark Brodie, and Sheng Ma. Top-cop: Mining top-k strongly correlated pairs in large databases. Data Mining, 2006. ICDM'06. Sixth International Conference on. IEEE, 2006.

%% Applications in machine learning
\bibitem{ml2}
Yu, Lei, and Huan Liu. Feature selection for high-dimensional data: A fast correlation-based filter solution. ICML. Vol. 3. 2003.

%% Other correlation pruning methods
\bibitem{prune3}
Zhang, Jian, and Joan Feigenbaum. Finding highly correlated pairs efficiently with powerful pruning. Proceedings of the 15th ACM international conference on Information and knowledge management. ACM, 2006.

\end{thebibliography}
\end{document}
